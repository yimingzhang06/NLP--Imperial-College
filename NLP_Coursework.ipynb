{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "NLP Coursework",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-G0phvwOefhT"
      },
      "source": [
        "#README\n",
        "\n",
        "**Author:** \\\n",
        "Yiming Zhang\\\n",
        "Ziyang Yang\\\n",
        "Lichen Xue\\\n",
        "\\\n",
        "**Description:**\\\n",
        "Humour, as a representative of human subjective judgment, is usually difficult to successfully predict its level through natural language processing methods. This notebook tries to predict humour levels of a sentence in 2 main approach, pre-trained and non pre-trained.\\\n",
        "**File structure:**\n",
        "\n",
        "\n",
        "*   import and download file\n",
        "*   Approach 1 base line\n",
        "  *   prepare before training\n",
        "  *   helper functions\n",
        "*   Approach 1\n",
        " *  data preprocessing & get all possible inputs\n",
        " *  load Tokenizer : Bert, XLNet, Roberta\n",
        " *  define three class for three pretrained dataset\n",
        " *  Train function\n",
        " *  Load the three pretrained model: Bert, XLNet, Roberta\n",
        " *  hyperparameters tuning (manually grid search :)\n",
        " *  Begin the train and save the model\n",
        " *  get the test RMSE and output predicted file\n",
        "*   Approach 2 base line\n",
        "*   Approach 2\n",
        " *  data preprocessing\n",
        " *  mini batch generating\n",
        " *  Training models: CNN, BiLSTM, GRU\n",
        " *  Training\n",
        " *  Testing\n",
        " *  Output predicted file\n",
        "\n",
        "\n",
        "\n",
        "**How to run the file:**\\\n",
        "run each cell from top to bottom. There should be no other extra file to add on. \\\n",
        "\n",
        "**How to save prediction file after run the model:**\\\n",
        "Save prediction file code has been comment out. If there is any need on get predict test score file, please remove the comments in \"output predictied file\" section in Approach 1 or 2 base need.\\\n",
        "To successfully save predicted file, you need to give the file access to your google drive, the access link will be shown as an ouput at the beginning of output file code.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNRTc3-Qe8Yi"
      },
      "source": [
        "# import and download what we need"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnGxSxedDH4_",
        "outputId": "e4b9480e-a4d6-420a-fc4a-03849ccb3114"
      },
      "source": [
        "# load transformers\n",
        "!pip install transformers\n",
        "!pip install SentencePiece #XLNET need SentencePiece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.3.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: SentencePiece in /usr/local/lib/python3.7/dist-packages (0.1.95)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHDtbLeue758",
        "outputId": "16440a43-2c34-4230-f921-3b4742f12b0c"
      },
      "source": [
        "import random\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "import torch.utils.data as tud\n",
        "\n",
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "from transformers import XLNetTokenizer\n",
        "from transformers import RobertaTokenizer\n",
        "\n",
        "from transformers import BertForSequenceClassification\n",
        "from transformers import XLNetForSequenceClassification\n",
        "from transformers import RobertaForSequenceClassification\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from torch.utils.data import Dataset, random_split\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import codecs\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import torch.utils.data as tud\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSOm_udTUQiN"
      },
      "source": [
        "# # You will need to download any word embeddings required for your code, e.g.:\n",
        "\n",
        "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "# !unzip glove.6B.zip\n",
        "\n",
        "# # For any packages that Colab does not provide auotmatically you will also need to install these below, e.g.:\n",
        "\n",
        "# ! pip install torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5Y5Gdoueah3",
        "outputId": "850e8e06-9413-4062-ea16-f08a663523a4"
      },
      "source": [
        "# Load data\n",
        "!wget -O train.csv https://drive.google.com/u/0/uc?id=1_jnO8KxostH_K98RrGg_7arJ7NA5RqtW&export=download\n",
        "!wget -O dev.csv https://drive.google.com/u/0/uc?id=13kfmFS9XXbrvWTlrN1C2W7hRMjhDQa5C&export=download\n",
        "!wget -O test.csv https://drive.google.com/u/0/uc?id=1v0XdNKw7gbrDUP3Xw9KaLPSGyX8w0fhk&export=download\n",
        "!wget -O test_trueLabel.csv https://drive.google.com/u/0/uc?id=16J7AfRi8awIXUHdpTHkgBga_Y4FVUKV5&export=download\n",
        "\n",
        "\n",
        "train_df = pd.read_csv('./train.csv')\n",
        "dev_df = pd.read_csv('./dev.csv')\n",
        "test_df = pd.read_csv('./test.csv')\n",
        "test_true = pd.read_csv('./test_trueLabel.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-02 10:12:01--  https://drive.google.com/u/0/uc?id=1_jnO8KxostH_K98RrGg_7arJ7NA5RqtW\n",
            "Resolving drive.google.com (drive.google.com)... 108.177.126.100, 108.177.126.101, 108.177.126.113, ...\n",
            "Connecting to drive.google.com (drive.google.com)|108.177.126.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0c-5g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/lm98c91rmjb4bj3f5h06b5dkrtka780m/1614679875000/02947314171395917638/*/1_jnO8KxostH_K98RrGg_7arJ7NA5RqtW [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-03-02 10:12:02--  https://doc-0c-5g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/lm98c91rmjb4bj3f5h06b5dkrtka780m/1614679875000/02947314171395917638/*/1_jnO8KxostH_K98RrGg_7arJ7NA5RqtW\n",
            "Resolving doc-0c-5g-docs.googleusercontent.com (doc-0c-5g-docs.googleusercontent.com)... 172.217.218.132, 2a00:1450:4013:c08::84\n",
            "Connecting to doc-0c-5g-docs.googleusercontent.com (doc-0c-5g-docs.googleusercontent.com)|172.217.218.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 947914 (926K) [text/csv]\n",
            "Saving to: ‘train.csv’\n",
            "\n",
            "train.csv           100%[===================>] 925.70K  --.-KB/s    in 0.007s  \n",
            "\n",
            "2021-03-02 10:12:03 (138 MB/s) - ‘train.csv’ saved [947914/947914]\n",
            "\n",
            "--2021-03-02 10:12:03--  https://drive.google.com/u/0/uc?id=13kfmFS9XXbrvWTlrN1C2W7hRMjhDQa5C\n",
            "Resolving drive.google.com (drive.google.com)... 108.177.126.113, 108.177.126.138, 108.177.126.100, ...\n",
            "Connecting to drive.google.com (drive.google.com)|108.177.126.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0g-5g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/pbr210iejlcg5jiastlqkre46egnna60/1614679875000/02947314171395917638/*/13kfmFS9XXbrvWTlrN1C2W7hRMjhDQa5C [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-03-02 10:12:04--  https://doc-0g-5g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/pbr210iejlcg5jiastlqkre46egnna60/1614679875000/02947314171395917638/*/13kfmFS9XXbrvWTlrN1C2W7hRMjhDQa5C\n",
            "Resolving doc-0g-5g-docs.googleusercontent.com (doc-0g-5g-docs.googleusercontent.com)... 172.217.218.132, 2a00:1450:4013:c08::84\n",
            "Connecting to doc-0g-5g-docs.googleusercontent.com (doc-0g-5g-docs.googleusercontent.com)|172.217.218.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 241339 (236K) [text/csv]\n",
            "Saving to: ‘dev.csv’\n",
            "\n",
            "dev.csv             100%[===================>] 235.68K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2021-03-02 10:12:04 (138 MB/s) - ‘dev.csv’ saved [241339/241339]\n",
            "\n",
            "--2021-03-02 10:12:04--  https://drive.google.com/u/0/uc?id=1v0XdNKw7gbrDUP3Xw9KaLPSGyX8w0fhk\n",
            "Resolving drive.google.com (drive.google.com)... 108.177.126.100, 108.177.126.101, 108.177.126.113, ...\n",
            "Connecting to drive.google.com (drive.google.com)|108.177.126.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0k-5g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/5lnb71tvgjprraublgp1bchk7abniklp/1614679875000/02947314171395917638/*/1v0XdNKw7gbrDUP3Xw9KaLPSGyX8w0fhk [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-03-02 10:12:05--  https://doc-0k-5g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/5lnb71tvgjprraublgp1bchk7abniklp/1614679875000/02947314171395917638/*/1v0XdNKw7gbrDUP3Xw9KaLPSGyX8w0fhk\n",
            "Resolving doc-0k-5g-docs.googleusercontent.com (doc-0k-5g-docs.googleusercontent.com)... 172.217.218.132, 2a00:1450:4013:c08::84\n",
            "Connecting to doc-0k-5g-docs.googleusercontent.com (doc-0k-5g-docs.googleusercontent.com)|172.217.218.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 269850 (264K) [text/csv]\n",
            "Saving to: ‘test.csv’\n",
            "\n",
            "test.csv            100%[===================>] 263.53K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2021-03-02 10:12:05 (135 MB/s) - ‘test.csv’ saved [269850/269850]\n",
            "\n",
            "--2021-03-02 10:12:05--  https://drive.google.com/u/0/uc?id=16J7AfRi8awIXUHdpTHkgBga_Y4FVUKV5\n",
            "Resolving drive.google.com (drive.google.com)... 108.177.126.100, 108.177.126.101, 108.177.126.138, ...\n",
            "Connecting to drive.google.com (drive.google.com)|108.177.126.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-08-5g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/adk8vgate95vfj3njahq178nocf67vio/1614679875000/02947314171395917638/*/16J7AfRi8awIXUHdpTHkgBga_Y4FVUKV5 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-03-02 10:12:06--  https://doc-08-5g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/adk8vgate95vfj3njahq178nocf67vio/1614679875000/02947314171395917638/*/16J7AfRi8awIXUHdpTHkgBga_Y4FVUKV5\n",
            "Resolving doc-08-5g-docs.googleusercontent.com (doc-08-5g-docs.googleusercontent.com)... 172.217.218.132, 2a00:1450:4013:c08::84\n",
            "Connecting to doc-08-5g-docs.googleusercontent.com (doc-08-5g-docs.googleusercontent.com)|172.217.218.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 297352 (290K) [text/csv]\n",
            "Saving to: ‘test_trueLabel.csv’\n",
            "\n",
            "test_trueLabel.csv  100%[===================>] 290.38K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2021-03-02 10:12:06 (106 MB/s) - ‘test_trueLabel.csv’ saved [297352/297352]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkZjVKdObmZ_",
        "outputId": "d9c83919-dfe9-488a-cb35-5380693161e8"
      },
      "source": [
        "#train_df.head()\n",
        "print(train_df.original)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0       France is ‘ hunting down its citizens who join...\n",
            "1       Pentagon claims 2,000 % increase in Russian tr...\n",
            "2       Iceland PM Calls Snap Vote as Pedophile Furor ...\n",
            "3       In an apparent first , Iran and Israel <engage...\n",
            "4       Trump was told weeks ago that Flynn misled <Vi...\n",
            "                              ...                        \n",
            "9647    State officials blast ' unprecedented ' DHS <m...\n",
            "9648    Protesters Rally for <Refugees/> Detained at J...\n",
            "9649    Cruise line Carnival Corp. joins the fight aga...\n",
            "9650    Columbia police hunt woman seen with <gun/> ne...\n",
            "9651    Here 's What 's In The House-Approved Health <...\n",
            "Name: original, Length: 9652, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hugqGsmLlOPm"
      },
      "source": [
        "# Baseline 1: prepare before training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4KX0h_ZlRSH"
      },
      "source": [
        "# Setting random seed and device\n",
        "SEED = 1\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# change it to a function\n",
        "def fix_seed(seed=2021):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "fix_seed()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvv4whUpJlkb",
        "outputId": "19d93d50-ce09-4fa8-fc74-27f1ca214d8a"
      },
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "DEVICE = device\n",
        "\n",
        "print('Device is', DEVICE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device is cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqk8A9QASbRg"
      },
      "source": [
        "# helper function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIeY9cvxStKk"
      },
      "source": [
        "## train and eval function for 1 input models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dl4PkNXe2xXP"
      },
      "source": [
        "def train1in(train_iter, dev_iter, model, number_epoch):\n",
        "    \"\"\"\n",
        "    Training loop for the model, which calls on eval to evaluate after each epoch\n",
        "    \"\"\"\n",
        "    \n",
        "    print(\"Training model.\")\n",
        "\n",
        "    for epoch in range(1, number_epoch+1):\n",
        "\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        epoch_sse = 0\n",
        "        no_observations = 0  # Observations used for training so far\n",
        "\n",
        "        for batch in train_iter:\n",
        "\n",
        "            feature1, target = batch\n",
        "\n",
        "            feature1, target = feature1.to(device),  target.to(device)\n",
        "\n",
        "            # for RNN:\n",
        "            model.batch_size = target.shape[0]\n",
        "            no_observations = no_observations + target.shape[0]\n",
        "            model.hidden = model.init_hidden()\n",
        "\n",
        "            predictions = model(feature1).squeeze(1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            loss = loss_fn(predictions, target)\n",
        "\n",
        "            sse, __ = model_performance(predictions.detach().cpu().numpy(), target.detach().cpu().numpy())\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()*target.shape[0]\n",
        "            epoch_sse += sse\n",
        "\n",
        "        valid_loss, valid_mse, __, __ = eval1in(dev_iter, model)\n",
        "\n",
        "        epoch_loss, epoch_mse = epoch_loss / no_observations, epoch_sse / no_observations\n",
        "        print(f'| Epoch: {epoch:02} | Train Loss: {epoch_loss:.2f} | Train MSE: {epoch_mse:.2f} | Train RMSE: {epoch_mse**0.5:.5f} | \\\n",
        "        Val. Loss: {valid_loss:.2f} | Val. MSE: {valid_mse:.2f} |  Val. RMSE: {valid_mse**0.5:.5f} |')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkESlXNU26RM"
      },
      "source": [
        "# We evaluate performance on our dev set\n",
        "def eval1in(data_iter, model):\n",
        "    \"\"\"\n",
        "    Evaluating model performance on the dev set\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    epoch_sse = 0\n",
        "    pred_all = []\n",
        "    trg_all = []\n",
        "    no_observations = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_iter:\n",
        "            feature1, target = batch\n",
        "\n",
        "            feature1, target = feature1.to(device), target.to(device)\n",
        "\n",
        "            # for RNN:\n",
        "            model.batch_size = target.shape[0]\n",
        "            no_observations = no_observations + target.shape[0]\n",
        "            model.hidden = model.init_hidden()\n",
        "\n",
        "            predictions = model(feature1).squeeze(1)\n",
        "            loss = loss_fn(predictions, target)\n",
        "\n",
        "            # We get the mse\n",
        "            pred, trg = predictions.detach().cpu().numpy(), target.detach().cpu().numpy()\n",
        "            sse, __ = model_performance(pred, trg)\n",
        "\n",
        "            epoch_loss += loss.item()*target.shape[0]\n",
        "            epoch_sse += sse\n",
        "            pred_all.extend(pred)\n",
        "            trg_all.extend(trg)\n",
        "\n",
        "    return epoch_loss/no_observations, epoch_sse/no_observations, np.array(pred_all), np.array(trg_all)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZwU06mZSo8K"
      },
      "source": [
        "## train & eval function for 2 inputs models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtZodHuXVH8G"
      },
      "source": [
        "def train2in(train_iter, dev_iter, model, number_epoch):\n",
        "    \"\"\"\n",
        "    Training loop for the model, which calls on eval to evaluate after each epoch\n",
        "    \"\"\"\n",
        "    optimizer = torch.optim.Adam(modelgru.parameters(),lr=0.09)\n",
        "\n",
        "    steps = 8\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, steps)\n",
        "    \n",
        "    print(\"Training model.\")\n",
        "\n",
        "    for epoch in range(1, number_epoch+1):\n",
        "\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        epoch_sse = 0\n",
        "        no_observations = 0  # Observations used for training so far\n",
        "\n",
        "        for batch in train_iter:\n",
        "\n",
        "            feature1, feature2, target = batch\n",
        "\n",
        "            feature1, feature2, target = feature1.to(device), feature2.to(device), target.to(device)\n",
        "\n",
        "            # for RNN:\n",
        "            model.batch_size = target.shape[0]\n",
        "            no_observations = no_observations + target.shape[0]\n",
        "            model.hidden = model.init_hidden()\n",
        "\n",
        "            predictions = model(feature1,feature2).squeeze(1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            loss = loss_fn(predictions, target)\n",
        "\n",
        "            sse, __ = model_performance(predictions.detach().cpu().numpy(), target.detach().cpu().numpy())\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            epoch_loss += loss.item()*target.shape[0]\n",
        "            epoch_sse += sse\n",
        "\n",
        "        valid_loss, valid_mse, __, __ = eval2in(dev_iter, model)\n",
        "\n",
        "        epoch_loss, epoch_mse = epoch_loss / no_observations, epoch_sse / no_observations\n",
        "        print(f'| Epoch: {epoch:02} | Train Loss: {epoch_loss:.2f} | Train MSE: {epoch_mse:.2f} | Train RMSE: {epoch_mse**0.5:.5f} | \\\n",
        "        Val. Loss: {valid_loss:.2f} | Val. MSE: {valid_mse:.2f} |  Val. RMSE: {valid_mse**0.5:.5f} |')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oAM4vz6VH8H"
      },
      "source": [
        "# We evaluate performance on our dev set\n",
        "def eval2in(data_iter, model):\n",
        "    \"\"\"\n",
        "    Evaluating model performance on the dev set\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    epoch_sse = 0\n",
        "    pred_all = []\n",
        "    trg_all = []\n",
        "    no_observations = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_iter:\n",
        "            feature1,feature2, target = batch\n",
        "\n",
        "            feature1,feature2, target = feature1.to(device), feature2.to(device), target.to(device)\n",
        "\n",
        "            # for RNN:\n",
        "            model.batch_size = target.shape[0]\n",
        "            no_observations = no_observations + target.shape[0]\n",
        "            model.hidden = model.init_hidden()\n",
        "\n",
        "            predictions = model(feature1, feature2).squeeze(1)\n",
        "            loss = loss_fn(predictions, target)\n",
        "\n",
        "            # We get the mse\n",
        "            pred, trg = predictions.detach().cpu().numpy(), target.detach().cpu().numpy()\n",
        "            sse, __ = model_performance(pred, trg)\n",
        "\n",
        "            epoch_loss += loss.item()*target.shape[0]\n",
        "            epoch_sse += sse\n",
        "            pred_all.extend(pred)\n",
        "            trg_all.extend(trg)\n",
        "\n",
        "    return epoch_loss/no_observations, epoch_sse/no_observations, np.array(pred_all), np.array(trg_all)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d9OoBdkS3rq"
      },
      "source": [
        "## other help functions\n",
        "\n",
        "\n",
        "*   model_performance\n",
        "*   create vocab\n",
        "*   collate_fn_padd\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TkhHrQqVH8H"
      },
      "source": [
        "# How we print the model performance\n",
        "def model_performance(output, target, print_output=False):\n",
        "    \"\"\"\n",
        "    Returns SSE and MSE per batch (printing the MSE and the RMSE)\n",
        "    \"\"\"\n",
        "\n",
        "    sq_error = (output - target)**2\n",
        "\n",
        "    sse = np.sum(sq_error)\n",
        "    mse = np.mean(sq_error)\n",
        "    rmse = np.sqrt(mse)\n",
        "\n",
        "    if print_output:\n",
        "        print(f'| MSE: {mse:.2f} | RMSE: {rmse:.5f} |')\n",
        "\n",
        "    return sse, mse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "felWAQRbVH8H"
      },
      "source": [
        "def create_vocab(data):\n",
        "    \"\"\"\n",
        "    Creating a corpus of all the tokens used\n",
        "    \"\"\" \n",
        "    tokenized_corpus = [] # Let us put the tokenized corpus in a list\n",
        "\n",
        "    for sentence in data:\n",
        "\n",
        "        tokenized_sentence = []\n",
        "\n",
        "        for token in sentence.split(' '): # simplest split is\n",
        "\n",
        "            tokenized_sentence.append(token)\n",
        "\n",
        "        tokenized_corpus.append(tokenized_sentence)\n",
        "\n",
        "    # Create single list of all vocabulary\n",
        "    vocabulary = []  # Let us put all the tokens (mostly words) appearing in the vocabulary in a list\n",
        "\n",
        "    for sentence in tokenized_corpus:\n",
        "\n",
        "        for token in sentence:\n",
        "\n",
        "            if token not in vocabulary:\n",
        "\n",
        "                if True:\n",
        "                    vocabulary.append(token)\n",
        "\n",
        "    return vocabulary, tokenized_corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikp220mjVH8I"
      },
      "source": [
        "def collate_fn_padd(batch):\n",
        "    '''\n",
        "    We add padding to our minibatches and create tensors for our model\n",
        "    '''\n",
        "\n",
        "    batch_labels = [l for f, l in batch]\n",
        "    batch_features = [f for f, l in batch]\n",
        "\n",
        "    batch_features_len = [len(f) for f, l in batch]\n",
        "\n",
        "    seq_tensor = torch.zeros((len(batch), max(batch_features_len))).long()\n",
        "\n",
        "    for idx, (seq, seqlen) in enumerate(zip(batch_features, batch_features_len)):\n",
        "        seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n",
        "\n",
        "    batch_labels = torch.FloatTensor(batch_labels)\n",
        "\n",
        "    return seq_tensor, batch_labels\n",
        "\n",
        "class Task1Dataset(Dataset):\n",
        "\n",
        "    def __init__(self, train_data, labels):\n",
        "        self.x_train = train_data\n",
        "        self.y_train = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y_train)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return self.x_train[item], self.y_train[item]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWv2AZleUDhJ"
      },
      "source": [
        "# Approach 1 Baseline\n",
        "\n",
        "> valid loss value : 0.58\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXAmVdjplr3k"
      },
      "source": [
        "# Approach 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJttf_ymo6vW"
      },
      "source": [
        "## data preprocessing & get all possible inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ld3sH7kIlxBW"
      },
      "source": [
        "#################################\n",
        "# Function : remove the punctuation in one sentence.\n",
        "# We do not use this function in approach 1.\n",
        "#################################\n",
        "import string\n",
        "def removePunctuation(text):\n",
        "    temp = []\n",
        "    for c in text:\n",
        "        if c not in string.punctuation:\n",
        "            temp.append(c)\n",
        "    newText = ''.join(temp)\n",
        "    return newText\n",
        "\n",
        "\n",
        "#################################\n",
        "# Function : remove the sentences without grade.\n",
        "# We do not use this function in approach 1.\n",
        "#################################\n",
        "def removeNonGradedRow (train_df):\n",
        "  train_df=train_df[~train_df['grades'].isin([0])]\n",
        "  return train_df\n",
        "\n",
        "\n",
        "\n",
        "#################################\n",
        "#Input: The result from pd.read_csv(train_file / dev_file / test_file)\n",
        "\n",
        "#Function: Extract and split five data from CSV: original sentences, new sentences, original words, new words, grade.\n",
        "# We do not remove the sentences which don't have grade, because the result with removing the no grading sentences is not better than not removing.\n",
        "\n",
        "#Output: Five list: original sentences, new sentences, original words, new words, grade.\n",
        "#################################\n",
        "\n",
        "def extract_data_in_csv_to_list(csv):\n",
        "    csv_data = [(origin_sentence, edit_word) for (origin_sentence, edit_word) in zip(csv.original.to_list(), csv.edit.to_list())]\n",
        "\n",
        "    #Get the grade from csv to list\n",
        "    grade = csv.meanGrade.to_list()\n",
        "\n",
        "    # list of tuple for original headlines and new edited headlines\n",
        "    original_sentences = []\n",
        "    new_sentences = []\n",
        "    original_words = []\n",
        "    edit_words = []\n",
        "\n",
        "    for original, edit in csv_data:\n",
        "\n",
        "      p = re.compile(r'\\<(.*?)\\/\\>')\n",
        "      #################################\n",
        "      # Firstly, we get the original words list and the edited words list\n",
        "      #get the original words and append to the list\n",
        "      original_word = ''.join(re.findall(p, original))\n",
        "      original_words.append(original_word)\n",
        "\n",
        "      #append the edited words to the list\n",
        "      edit_words.append(edit)\n",
        "      #################################\n",
        "\n",
        "      #################################\n",
        "      # Secondly, we get the original sentences list and the new sentences list\n",
        "      #get the original sentences and append to the list\n",
        "      original_sentence = p.sub(original_word, original)\n",
        "      original_sentences.append(original_sentence)\n",
        "\n",
        "      #get the new sentences and append to the list\n",
        "      new_sentence = p.sub(edit, original)\n",
        "      new_sentences.append(new_sentence)\n",
        "      #################################\n",
        "\n",
        "    return original_sentences, new_sentences, original_words, edit_words, grade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8vUfcMMFQcR",
        "outputId": "79145019-6dc0-474f-899b-70b545ecc5a7"
      },
      "source": [
        "#################################\n",
        "## extract the data from csv to list\n",
        "## train_df -> 4 list\n",
        "## dev_df -> 4 list\n",
        "## test_df -> 4 list\n",
        "#################################\n",
        "\n",
        "train_original_sentences, train_new_sentences, train_original_words, train_edit_words, train_grade_list = extract_data_in_csv_to_list(train_df)\n",
        "valid_original_sentences, valid_new_sentences, valid_original_words, valid_edit_words, valid_grade_list = extract_data_in_csv_to_list(dev_df)\n",
        "test_original_sentences, test_new_sentences, test_original_words, test_edit_words, test_grade_list = extract_data_in_csv_to_list(test_true)\n",
        "\n",
        "len(train_original_sentences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9652"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMDomZhFGcUr",
        "outputId": "909aec52-558e-4aca-f35e-9dc814f85911"
      },
      "source": [
        "##check the max length\n",
        "\n",
        "max_length = 0\n",
        "for sent in train_original_sentences:\n",
        "    sent = sent.split()\n",
        "    max_length = max(len(sent), max_length)\n",
        "print('the max is', max_length)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the max is 26\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoWz3a3ypD6g"
      },
      "source": [
        "## load Tokenizer\n",
        "BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGo_gL1Gl1gF"
      },
      "source": [
        "########################################################################################################################################\n",
        "\n",
        "## There are some Tokenizer choices, we use Bert as out first choice. ##\n",
        "\n",
        "########################################################################################################################################\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "# Load the BERT tokenizer.\n",
        "tokenizer_bert = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "train_inputs_two_sentences_bert = tokenizer_bert(train_original_sentences, train_new_sentences, padding='max_length', max_length=60, truncation=True, return_tensors=\"pt\")\n",
        "valid_inputs_two_sentences_bert = tokenizer_bert(valid_original_sentences, valid_new_sentences, padding='max_length', max_length=60, truncation=True, return_tensors=\"pt\")\n",
        "test_inputs_two_sentences_bert = tokenizer_bert(test_original_sentences, test_new_sentences, padding='max_length', max_length=60, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "train_inputs_new_sen_new_word_bert = tokenizer_bert(train_new_sentences, train_edit_words, padding='max_length', max_length=35, truncation=True, return_tensors=\"pt\")\n",
        "valid_inputs_new_sen_new_word_bert = tokenizer_bert(valid_new_sentences, valid_edit_words, padding='max_length', max_length=35, truncation=True, return_tensors=\"pt\")\n",
        "test_inputs_new_sen_new_word_bert = tokenizer_bert(test_new_sentences, test_edit_words, padding='max_length', max_length=35, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "train_inputs_ori_sen_new_word_bert = tokenizer_bert(train_original_sentences, train_edit_words, padding='max_length', max_length=35, truncation=True, return_tensors=\"pt\")\n",
        "valid_inputs_ori_sen_new_word_bert = tokenizer_bert(valid_original_sentences, valid_edit_words, padding='max_length', max_length=35, truncation=True, return_tensors=\"pt\")\n",
        "test_inputs_ori_sen_new_word_bert = tokenizer_bert(test_original_sentences, test_edit_words, padding='max_length', max_length=35, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "train_inputs_new_sen_ori_word_bert = tokenizer_bert(train_new_sentences, train_original_words, padding='max_length', max_length=35, truncation=True, return_tensors=\"pt\")\n",
        "valid_inputs_new_sen_ori_word_bert = tokenizer_bert(valid_new_sentences, valid_original_words, padding='max_length', max_length=35, truncation=True, return_tensors=\"pt\")\n",
        "test_inputs_new_sen_ori_word_bert = tokenizer_bert(test_new_sentences, test_original_words, padding='max_length', max_length=35, truncation=True, return_tensors=\"pt\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfuuhMJOG0al"
      },
      "source": [
        "XLNET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTgMT84dl53M"
      },
      "source": [
        "## XLNet Tokenizer ##\n",
        "\n",
        "########################################################################################################################################\n",
        "\n",
        "## Some times the XLNet will report an bug \"NoneType\", please do not worry, it is just because of the Colab issue. You can restart the Colab. ##\n",
        "\n",
        "########################################################################################################################################\n",
        "\n",
        "\n",
        "from transformers import XLNetTokenizer\n",
        "\n",
        "tokenizer_xlnet = XLNetTokenizer.from_pretrained('xlnet-base-cased', do_lower_case=True)\n",
        "\n",
        "train_inputs_two_sentences_xlnet = tokenizer_xlnet(train_original_sentences, train_new_sentences, padding='max_length', max_length=60, truncation=True, return_tensors=\"pt\")\n",
        "valid_inputs_two_sentences_xlnet = tokenizer_xlnet(valid_original_sentences, valid_new_sentences, padding='max_length', max_length=60, truncation=True, return_tensors=\"pt\")\n",
        "test_inputs_two_sentences_xlnet = tokenizer_xlnet(test_original_sentences, test_new_sentences, padding='max_length', max_length=60, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "train_inputs_new_sen_new_word_xlnet = tokenizer_xlnet(train_new_sentences, train_edit_words, padding='max_length', max_length=35, truncation=True, return_tensors=\"pt\")\n",
        "valid_inputs_new_sen_new_word_xlnet = tokenizer_xlnet(valid_new_sentences, valid_edit_words, padding='max_length', max_length=35, truncation=True, return_tensors=\"pt\")\n",
        "test_inputs_new_sen_new_word_xlnet = tokenizer_xlnet(test_new_sentences, test_edit_words, padding='max_length', max_length=35, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "train_inputs_ori_sen_new_word_xlnet = tokenizer_xlnet(train_original_sentences, train_edit_words, padding='max_length', max_length=35, truncation=True, return_tensors=\"pt\")\n",
        "valid_inputs_ori_sen_new_word_xlnet = tokenizer_xlnet(valid_original_sentences, valid_edit_words, padding='max_length', max_length=35, truncation=True, return_tensors=\"pt\")\n",
        "test_inputs_ori_sen_new_word_xlnet = tokenizer_xlnet(test_original_sentences, test_edit_words, padding='max_length', max_length=35, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "train_inputs_new_sen_ori_word_xlnet = tokenizer_xlnet(train_new_sentences, train_original_words, padding='max_length', max_length=35, truncation=True, return_tensors=\"pt\")\n",
        "valid_inputs_new_sen_ori_word_xlnet = tokenizer_xlnet(valid_new_sentences, valid_original_words, padding='max_length', max_length=35, truncation=True, return_tensors=\"pt\")\n",
        "test_inputs_new_sen_ori_word_xlnet = tokenizer_xlnet(test_new_sentences, test_original_words, padding='max_length', max_length=35, truncation=True, return_tensors=\"pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSIWiFRfHy0-"
      },
      "source": [
        "ROBERTA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-pYHN1GHtqQ"
      },
      "source": [
        "## Roberta Tokenizer ##\n",
        "\n",
        "from transformers import RobertaTokenizer \n",
        "\n",
        "tokenizer_roberta = RobertaTokenizer.from_pretrained('roberta-base', do_lower_case=True)\n",
        "\n",
        "train_inputs_two_sentences_roberta = tokenizer_roberta(train_original_sentences, train_new_sentences, padding='max_length', max_length=60, truncation=True, return_tensors=\"pt\")\n",
        "valid_inputs_two_sentences_roberta = tokenizer_roberta(valid_original_sentences, valid_new_sentences, padding='max_length', max_length=60, truncation=True, return_tensors=\"pt\")\n",
        "test_inputs_two_sentences_roberta = tokenizer_roberta(test_original_sentences, test_new_sentences, padding='max_length', max_length=60, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "train_inputs_new_sen_new_word_roberta = tokenizer_roberta(train_new_sentences, train_edit_words, padding='max_length', max_length=35, truncation=True, return_tensors=\"pt\")\n",
        "valid_inputs_new_sen_new_word_roberta = tokenizer_roberta(valid_new_sentences, valid_edit_words, padding='max_length', max_length=35, truncation=True, return_tensors=\"pt\")\n",
        "test_inputs_new_sen_new_word_roberta = tokenizer_roberta(test_new_sentences, test_edit_words, padding='max_length', max_length=35, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "train_inputs_ori_sen_new_word_roberta = tokenizer_roberta(train_original_sentences, train_edit_words, padding='max_length', max_length=35, truncation=True, return_tensors=\"pt\")\n",
        "valid_inputs_ori_sen_new_word_roberta = tokenizer_roberta(valid_original_sentences, valid_edit_words, padding='max_length', max_length=35, truncation=True, return_tensors=\"pt\")\n",
        "test_inputs_ori_sen_new_word_roberta = tokenizer_roberta(test_original_sentences, test_edit_words, padding='max_length', max_length=35, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "train_inputs_new_sen_ori_word_roberta = tokenizer_roberta(train_new_sentences, train_original_words, padding='max_length', max_length=35, truncation=True, return_tensors=\"pt\")\n",
        "valid_inputs_new_sen_ori_word_roberta = tokenizer_roberta(valid_new_sentences, valid_original_words, padding='max_length', max_length=35, truncation=True, return_tensors=\"pt\")\n",
        "test_inputs_new_sen_ori_word_roberta = tokenizer_roberta(test_new_sentences, test_original_words, padding='max_length', max_length=35, truncation=True, return_tensors=\"pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5dOnlf3IIbT"
      },
      "source": [
        "Get the tensor data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3Dv74_Ll868"
      },
      "source": [
        "###############################################################################################################################################################\n",
        "\n",
        "## For different inputs, you could change the <train_inputs_two_sentences_bert> to any other input. Such as <train_inputs_new_sen_new_word_bert>.\n",
        "\n",
        "###############################################################################################################################################################\n",
        "\n",
        "\n",
        "#####################################################################################################################################################################################################\n",
        "\n",
        "## Another warm remind, reborta does not have the \"token_type_ids\", only have the \"input_ids\" and \"attention_mask\". So please update the code if you want to try the roberta.\n",
        "\n",
        "#####################################################################################################################################################################################################\n",
        "\n",
        "\n",
        "train_ids = train_inputs_two_sentences_bert['input_ids']\n",
        "train_attention = train_inputs_two_sentences_bert['attention_mask']\n",
        "train_token = train_inputs_two_sentences_bert['token_type_ids'] # need to delete it for reborta\n",
        "train_grade = torch.tensor(train_grade_list)\n",
        "\n",
        "valid_ids = valid_inputs_two_sentences_bert['input_ids']\n",
        "valid_attention = valid_inputs_two_sentences_bert['attention_mask']\n",
        "valid_token = valid_inputs_two_sentences_bert['token_type_ids'] # need to delete it for reborta\n",
        "valid_grade = torch.tensor(valid_grade_list)\n",
        "\n",
        "test_ids = test_inputs_two_sentences_bert['input_ids']\n",
        "test_attention = test_inputs_two_sentences_bert['attention_mask']\n",
        "test_token = test_inputs_two_sentences_bert['token_type_ids'] # need to delete it for reborta\n",
        "test_grade = torch.tensor(test_grade_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vfeNZTnI-3C"
      },
      "source": [
        "# Three class for bert, xlnet, roberta data. Generate the data loaders."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_wfZMI9Jzxm"
      },
      "source": [
        "bert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfyP52Ifl-f8"
      },
      "source": [
        "#######################################\n",
        "\n",
        "## prepare for bert training. ##\n",
        "\n",
        "#######################################\n",
        "\n",
        "class bert_data(tud.Dataset):\n",
        "    def __init__(self, id, mask, token, grade):\n",
        "        self.length = id.shape[0]\n",
        "        self.id = id.to(DEVICE)\n",
        "        self.mask = mask.to(DEVICE)\n",
        "        self.token = token.to(DEVICE)\n",
        "        self.grade = grade.to(DEVICE)\n",
        "    def __getitem__(self, index):\n",
        "        return self.id[index], self.mask[index], self.token[index], self.grade[index]\n",
        "    def __len__(self):\n",
        "        return self.length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKe4pfPIJ0xr"
      },
      "source": [
        "xlnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBQLeZ4dIRRb"
      },
      "source": [
        "#######################################\n",
        "\n",
        "## prepare for xlnet training. ##\n",
        "\n",
        "#######################################\n",
        "\n",
        "class xlnet_data(tud.Dataset):\n",
        "    def __init__(self, id, mask, token, grade):\n",
        "        self.length = id.shape[0]\n",
        "        self.id = id.to(DEVICE)\n",
        "        self.mask = mask.to(DEVICE)\n",
        "        self.token = token.to(DEVICE)\n",
        "        self.grade = grade.to(DEVICE)\n",
        "    def __getitem__(self, index):\n",
        "        return self.id[index], self.mask[index], self.token[index], self.grade[index]\n",
        "    def __len__(self):\n",
        "        return self.length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ob4ytvVmJ2aM"
      },
      "source": [
        "roberta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pfsVX4zITqv"
      },
      "source": [
        "#######################################\n",
        "\n",
        "## prepare for roberta training. ##\n",
        "\n",
        "#######################################\n",
        "class roberta_Dataset(tud.Dataset):\n",
        "    def __init__(self, id, mask, grade): \n",
        "        self.length = id.shape[0]\n",
        "        self.id = id.to(DEVICE)\n",
        "        self.mask = mask.to(DEVICE)\n",
        "        self.grade = grade.to(DEVICE)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "       return self.id[index], self.mask[index], self.grade[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj0HMhPVJQ3h"
      },
      "source": [
        "get the loader for train, validation, test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08ROxoeem9cw"
      },
      "source": [
        "####################################################################################\n",
        "\n",
        "## warm reminder : you need delete token for reborta testing if you want.##\n",
        "\n",
        "####################################################################################\n",
        "\n",
        "##set batch size and get the three loader.\n",
        "\n",
        "fix_seed()\n",
        "\n",
        "Batch_size = 16\n",
        "train_dataset = bert_data(train_ids, train_attention, train_token, train_grade) \n",
        "valid_dataset = bert_data(valid_ids, valid_attention, valid_token, valid_grade)\n",
        "test_dataset = bert_data(test_ids, test_attention, test_token, test_grade)\n",
        "\n",
        "train_loader = tud.DataLoader(train_dataset, batch_size=Batch_size, shuffle=True)\n",
        "valid_loader = tud.DataLoader(valid_dataset, batch_size=Batch_size, shuffle=True)\n",
        "test_loader = tud.DataLoader(test_dataset, batch_size=Batch_size, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RebJ21RJ50S"
      },
      "source": [
        "# Train function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ftaz0-rnfTk"
      },
      "source": [
        "## The train function ##\n",
        "#warm reminding : you need delete token for reborta testing :) \n",
        "\n",
        "def train(epochs, optimizer, loss_f, model, train_loader, valid_loader, scheduler):\n",
        "    fix_seed()\n",
        "    model = model.to(DEVICE)\n",
        "\n",
        "    for n in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for ids_batch, mask_batch, token_batch, grade in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(ids_batch,attention_mask=mask_batch,token_type_ids=token_batch)\n",
        "            pre_result = outputs[0].squeeze(1) # get the prediction result\n",
        "\n",
        "            loss = loss_f(pre_result, grade)\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()  # update the parameters \n",
        "            scheduler.step() # update the learning rate dynamicly.\n",
        "            \n",
        "            train_loss += loss.item()\n",
        "        train_epoch_loss = train_loss / len(train_loader)\n",
        "        \n",
        "        val_loss = 0\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for ids_batch, mask_batch, token_batch, grade in valid_loader:\n",
        "                outputs = model(ids_batch,attention_mask=mask_batch,token_type_ids=token_batch)\n",
        "                pre_result = outputs[0].squeeze(1)\n",
        "                loss = loss_f(pre_result, grade)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        val_epoch_loss = val_loss / len(valid_loader)\n",
        "\n",
        "\n",
        "        print(\"\\nBegin the next epoch...\")\n",
        "        print(f'Epoch: {(n+1)}')\n",
        "        print(f'Train Loss: {train_epoch_loss:.5f}')\n",
        "        print(f'Val Loss: {val_epoch_loss:.5f}')\n",
        "\n",
        "    print(\"\\nOver!\")\n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvxHbqhWJ_b2"
      },
      "source": [
        "# Load the pre-trained model\n",
        "Bert : Bert For Sequence Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcB0S5wLmAJs",
        "outputId": "113626cd-73f0-43c9-a817-cbb954dae231"
      },
      "source": [
        "###########\n",
        "\n",
        "# we use bert to get the best result. \n",
        "\n",
        "###########\n",
        "\n",
        "## just import again for emphasizing\n",
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "# Load bert\n",
        "model_bert = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",num_labels = 1, output_attentions = False, output_hidden_states = False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVriwjKhKPfk"
      },
      "source": [
        "XLNet: XLNet For Sequence Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45c9yt7KKOyO",
        "outputId": "9cfb7621-fe3f-4d88-ee1b-abc50484bac9"
      },
      "source": [
        "###########\n",
        "\n",
        "# You could try the Xlnet if you want, the result is not good. \n",
        "# You might also need a <colab pro> account to run this code. :)\n",
        "# Please press \"Ctrl + A\", and \"Ctrl + /\", if you do not want it.\n",
        "\n",
        "###########\n",
        "\n",
        "## just import again for emphasizing\n",
        "from transformers import XLNetForSequenceClassification\n",
        "\n",
        "# Load XLNet\n",
        "model_xlnet = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\",num_labels = 1, output_attentions = False, output_hidden_states = False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
            "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7rA6qxyKY1P"
      },
      "source": [
        "Roberta : Roberta For Sequence Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZEdFqeNKX-e",
        "outputId": "16a0ccb1-2fb4-41a1-e4d5-150be5dcd17c"
      },
      "source": [
        "###########\n",
        "\n",
        "# You could try the RoBerta if you want, the result is not good. \n",
        "# Please press \"Ctrl + A\", and \"Ctrl + /\", if you do not want it.\n",
        "\n",
        "###########\n",
        "\n",
        "## just import again for emphasizing\n",
        "from transformers import RobertaForSequenceClassification\n",
        "\n",
        "# Load Roberta\n",
        "model_roberta = RobertaForSequenceClassification.from_pretrained(\"roberta-base\",num_labels = 1, output_attentions = False, output_hidden_states = False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHHF-BNoKv8H"
      },
      "source": [
        "# Set the hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnhAgKKZK06X"
      },
      "source": [
        "epoch, learning rate, warmup, weight decay."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHYYI8Z9K0SB"
      },
      "source": [
        "# According to the Bert manual.\n",
        "\n",
        "epoch = 2\n",
        "bert_lr = 1e-2\n",
        "\n",
        "## warm up ##\n",
        "warmup = 0.3\n",
        "train_step = len(train_loader) * epoch * 2\n",
        "warmup_step = int(train_step * warmup)\n",
        "\n",
        "## weightdecay ##\n",
        "weightdecay = 0\n",
        "optimizer_grouped_parameters = [\n",
        "   {'params': [p for n, p in model_bert.named_parameters() if \"bert\" not in n], 'lr': bert_lr, 'weight_decay': weightdecay},\n",
        "   {'params': [p for n, p in model_bert.named_parameters() if \"bert\" in n], 'weight_decay': weightdecay}\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqjSmlizLDAU"
      },
      "source": [
        "optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbGe_S9kK_O0"
      },
      "source": [
        "## optimizer : AdamW from transformer ##\n",
        "\n",
        "bert_eps = 1e-8\n",
        "bert_fr = 2e-5\n",
        "# eps is used to avoid zero division nightmare. AdamW is very useful.\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=bert_fr, eps = bert_eps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jZuqXmRLHJd"
      },
      "source": [
        "scheduler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMI1tJLELGA9"
      },
      "source": [
        "## scheduler ##\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = warmup_step, num_training_steps = train_step)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOIk6bUgLJkJ"
      },
      "source": [
        "loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QoAdleuLIOj"
      },
      "source": [
        "## loss function ##\n",
        "def RMSE(predictions, labels):\n",
        "    loss = torch.sqrt(((predictions - labels)**2).mean())\n",
        "    return loss\n",
        "\n",
        "loss_function = RMSE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P62V3gNdLMnb"
      },
      "source": [
        "# begin training!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eajxkd_cLK82",
        "outputId": "e157a292-c27f-4067-a914-77b94b5f2f6a"
      },
      "source": [
        "## Begin our training ! ##\n",
        "\n",
        "train(epoch, optimizer,  loss_function,  model_bert, train_loader, valid_loader,  scheduler)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Begin the next epoch...\n",
            "Epoch: 1\n",
            "Train Loss: 0.59640\n",
            "Val Loss: 0.52932\n",
            "\n",
            "Begin the next epoch...\n",
            "Epoch: 2\n",
            "Train Loss: 0.53617\n",
            "Val Loss: 0.51369\n",
            "\n",
            "Over!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhrOJjhiLT7V"
      },
      "source": [
        "Save model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_Kd3XwTLSnp"
      },
      "source": [
        "## save model! ##\n",
        "torch.save(model_bert, '\\model_bert.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WufeghizLZIz"
      },
      "source": [
        "# Check the test RMSE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nK9HCdyLYfv",
        "outputId": "4833ad8f-d685-479b-f38b-4a9849eb1d88"
      },
      "source": [
        "### load model!##\n",
        "##model_bert = torch.load('\\model_bert.pkl')\n",
        "\n",
        "## testing! ##\n",
        "fix_seed()\n",
        "model_bert.eval()\n",
        "\n",
        "test_ids = test_ids.to(DEVICE)\n",
        "test_attention = test_attention.to(DEVICE)\n",
        "test_token = test_token.to(DEVICE)\n",
        "test_grade = test_grade.to(DEVICE)\n",
        "\n",
        "with torch.no_grad():\n",
        "  pre_result = model_bert(test_ids,attention_mask=test_attention,token_type_ids=test_token)[0].squeeze(1)\n",
        "  test_loss = torch.sqrt(((pre_result - test_grade)**2).mean()).item()\n",
        "print(f'Test RMSE: {test_loss:.5f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test RMSE: 0.51760\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fErHJkPZLgqz"
      },
      "source": [
        "write the result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DXFPWuPLfli",
        "outputId": "8f13fe64-72a0-479f-8a25-056d39bc8cd6"
      },
      "source": [
        "## write down! ##\n",
        "def write_down(predictions, test_data_frame, out_loc):\n",
        "    test_data_frame['pred'] = predictions\n",
        "    output = test_data_frame[['id','pred']]\n",
        "    output.to_csv(out_loc, index=False)\n",
        "    print('output finished, address: '+os.path.abspath(out_loc))\n",
        "\n",
        "# write test data result to 'task-1-output.csv'\n",
        "address = 'task-1-output.csv'\n",
        "\n",
        "## <test> is the result from <test = pd.read_csv(csv_file_of_test)>\n",
        "write_down(pre_result.cpu(), test_true, address)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "output finished, address: /content/task-1-output.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDsAezt2KLSa"
      },
      "source": [
        "## bert dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2oD90ooUYu9"
      },
      "source": [
        "# Approach 2 Baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKw0z5iXUXuq",
        "outputId": "d0d05fac-bbc4-4929-bf21-566927f71666"
      },
      "source": [
        "# Number of epochs\n",
        "epochs = 10\n",
        "\n",
        "# Proportion of training data for train compared to dev\n",
        "train_proportion = 0.8\n",
        "\n",
        "train_and_dev = train_df['edit']\n",
        "\n",
        "training_data, dev_data, training_y, dev_y = train_test_split(train_df['edit'], train_df['meanGrade'],\n",
        "                                                                        test_size=(1-train_proportion),\n",
        "                                                                        random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "# We train a Tf-idf model\n",
        "count_vect = CountVectorizer(stop_words='english')\n",
        "train_counts = count_vect.fit_transform(training_data)#just use the edit to predict the output grade.\n",
        "transformer = TfidfTransformer().fit(train_counts)\n",
        "train_counts = transformer.transform(train_counts)\n",
        "regression_model = LinearRegression().fit(train_counts, training_y)\n",
        "\n",
        "# Train predictions\n",
        "predicted_train = regression_model.predict(train_counts)\n",
        "\n",
        "# Calculate Tf-idf using train and dev, and validate model on dev:\n",
        "test_and_test_counts = count_vect.transform(train_and_dev)\n",
        "transformer = TfidfTransformer().fit(test_and_test_counts)\n",
        "\n",
        "test_counts = count_vect.transform(dev_data)\n",
        "\n",
        "test_counts = transformer.transform(test_counts)\n",
        "\n",
        "# Dev predictions\n",
        "predicted = regression_model.predict(test_counts)\n",
        "\n",
        "# We run the evaluation:\n",
        "print(\"\\nTrain performance:\")\n",
        "sse, mse = model_performance(predicted_train, training_y, True)\n",
        "\n",
        "print(\"\\nDev performance:\")\n",
        "sse, mse = model_performance(predicted, dev_y, True)\n",
        "\n",
        "\n",
        "test_countsT = count_vect.transform(test_df['edit'])\n",
        "test_countsT = transformer.transform(test_countsT)\n",
        "predictedT = regression_model.predict(test_countsT)\n",
        "print(predictedT)\n",
        "print(type(predictedT))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train performance:\n",
            "| MSE: 0.13 | RMSE: 0.36666 |\n",
            "\n",
            "Dev performance:\n",
            "| MSE: 0.36 | RMSE: 0.60400 |\n",
            "[0.46666667 0.93586957 0.93586957 ... 0.93586957 1.15555554 0.93586957]\n",
            "<class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-gplAHZLYrC"
      },
      "source": [
        "# Approach 2 Non-Pretrained models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJVns0XBOJl3"
      },
      "source": [
        "## data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBcxK5HONpyH"
      },
      "source": [
        "### helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bFB4s5yL0qk"
      },
      "source": [
        "##############################################################\n",
        "# remove all sentences which have 0 score in training set\n",
        "##############################################################\n",
        "\n",
        "def removeNonGradedRow (train_df):\n",
        "  train_df=train_df[~train_df['grades'].isin([0])]\n",
        "  return train_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AguvKy-jNz-g"
      },
      "source": [
        "##############################################################\n",
        "# split the dataset based on \" \"\n",
        "# lower case all letters in the original sentences\n",
        "##############################################################\n",
        "def create_vocab_approach2 (data):\n",
        "    \"\"\"\n",
        "    Creating a corpus of all the tokens used\n",
        "    \"\"\" \n",
        "    tokenized_corpus = [] # Let us put the tokenized corpus in a list\n",
        "    for sentence in data:\n",
        "        tokenized_sentence = []\n",
        "        for token in sentence.split(' '): # simplest split is\n",
        "            token = token.lower()\n",
        "            tokenized_sentence.append(token)\n",
        "        tokenized_corpus.append(tokenized_sentence)\n",
        "    return tokenized_corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Sp64I24OSw5"
      },
      "source": [
        "##############################################################\n",
        "# get word to idx list for further training\n",
        "##############################################################\n",
        "def word2idx_approach2(joint_tokenized_corpus, trainEdit):\n",
        "    vocabulary = []\n",
        "    for sentence in joint_tokenized_corpus:\n",
        "      for word in sentence:\n",
        "          if word not in vocabulary:\n",
        "              vocabulary.append(word)\n",
        "              \n",
        "    for word in trainEdit:\n",
        "      if word not in vocabulary:\n",
        "          vocabulary.append(word)\n",
        "  \n",
        "    word2idx = {w: idx+1 for (idx, w) in enumerate(vocabulary)}\n",
        "    word2idx['<pad>'] = 0\n",
        "    \n",
        "    return word2idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPl2ykvxOVCP"
      },
      "source": [
        "##############################################################\n",
        "# replace original sentences with edit words\n",
        "# remove putuations for the new edited sentences\n",
        "##############################################################\n",
        "\n",
        "def replaceEdited(joint_tokenized_corpus, trainEdit):\n",
        "    editedSen = joint_tokenized_corpus\n",
        "    for i in range(0,len(editedSen)):\n",
        "      for j in range(0,len(editedSen[i])):\n",
        "        editedSen[i][j] = re.sub('<.*>', trainEdit[i], editedSen[i][j])\n",
        "        editedSen[i][j] = re.sub(r'\\W+', '', editedSen[i][j])\n",
        "    return editedSen"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0XSxJsZOWfw"
      },
      "source": [
        "##############################################################\n",
        "# remove putuations for any input list\n",
        "##############################################################\n",
        "def removeSymbols(d):\n",
        "    for i in range(0,len(d)):\n",
        "      for j in range(0,len(d[i])):\n",
        "        d[i][j] = re.sub(r'\\W+', '', d[i][j])\n",
        "    return d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la345Be9yOEk"
      },
      "source": [
        "##############################################################\n",
        "# Use Tokenizer to pad all sentence to same length\n",
        "# convert input numpy array to tensor for output\n",
        "##############################################################\n",
        "\n",
        "def data2tensor(trainOriginal, editedSen):\n",
        "    # Encode strings into integers\n",
        "    tokenizer = Tokenizer()\n",
        "\n",
        "    # create vocabulary from all words\n",
        "    tokenizer.fit_on_texts(trainOriginal)\n",
        "    tokenizer.fit_on_texts(editedSen)\n",
        "\n",
        "    # get length (word count) of the longest row\n",
        "    maxLength = max([len(x) for x in trainOriginal])\n",
        "\n",
        "    # convert words into integers\n",
        "    w2iOrg = tokenizer.texts_to_sequences(trainOriginal)\n",
        "    w2iEdit = tokenizer.texts_to_sequences(editedSen)\n",
        "\n",
        "    #pad shorter rows' missing with zeros\n",
        "    padOrg = pad_sequences(w2iOrg, maxlen=maxLength, padding='post')\n",
        "    padEdit = pad_sequences(w2iEdit, maxlen=maxLength, padding='post')\n",
        "\n",
        "    tensorOrg = torch.from_numpy(padOrg)\n",
        "    tensorEdit = torch.from_numpy(padEdit)\n",
        "\n",
        "    return tensorOrg,tensorEdit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFwpTMYiObKp"
      },
      "source": [
        "### preprocessing Train,Valid,Test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3j8vRhiQOgzh"
      },
      "source": [
        "train_df = removeNonGradedRow(train_df)\n",
        "\n",
        "x = train_df.loc[:, [\"original\", \"edit\"]]\n",
        "y = train_df.loc[:, [\"meanGrade\"]]\n",
        "Xtrain, Xvalid, Ytrain, Yvalid = train_test_split(x, y, test_size=0.2)\n",
        "\n",
        "trainOriginal = Xtrain['original']\n",
        "validOriginal = Xvalid['original']\n",
        "testOriginal = test_true['original']\n",
        "\n",
        "trainEdit = Xtrain['edit'].values.tolist()\n",
        "validEdit = Xvalid['edit'].values.tolist()\n",
        "testEdit= test_true['edit'].values.tolist()\n",
        "\n",
        "trainGrade_lst = Ytrain['meanGrade'].to_list()\n",
        "validGrade_lst = Yvalid['meanGrade'].to_list()\n",
        "testGrade_lst = test_true['meanGrade'].to_list()\n",
        "\n",
        "#preprocessing for training data.\n",
        "replaceSen = create_vocab_approach2(trainOriginal) \n",
        "keepOrgianlSen = create_vocab_approach2(trainOriginal)\n",
        "\n",
        "word2idx = word2idx_approach2(create_vocab_approach2(trainOriginal), trainEdit)\n",
        "editedSen = replaceEdited(replaceSen, trainEdit)\n",
        "orgianlSen = removeSymbols(keepOrgianlSen)\n",
        "\n",
        "orgTensor,editTensor = data2tensor(orgianlSen, editedSen)\n",
        "labelTensor = torch.FloatTensor(trainGrade_lst)\n",
        "orgTensor,editTensor=orgTensor.type(torch.LongTensor),editTensor.type(torch.LongTensor)\n",
        "\n",
        "\n",
        "#pre processing for valid data. V means validation.\n",
        "replaceSenV = create_vocab_approach2(validOriginal)\n",
        "keepOrgianlSenV = create_vocab_approach2(validOriginal)\n",
        "\n",
        "editedSenV = replaceEdited(replaceSenV, validEdit)\n",
        "orgianlSenV = removeSymbols(keepOrgianlSenV)\n",
        "\n",
        "orgTensorV,editTensorV = data2tensor(orgianlSenV, editedSenV)\n",
        "labelTensorV = torch.FloatTensor(validGrade_lst)\n",
        "orgTensorV,editTensorV=orgTensorV.type(torch.LongTensor),editTensorV.type(torch.LongTensor)\n",
        "\n",
        "\n",
        "#pre processing for test data. T means Test.\n",
        "replaceSenT = create_vocab_approach2(testOriginal)\n",
        "keepOrgianlSenT = create_vocab_approach2(testOriginal)\n",
        "\n",
        "editedSenT = replaceEdited(replaceSenT, testEdit)\n",
        "orgianlSenT = removeSymbols(keepOrgianlSenT)\n",
        "\n",
        "orgTensorT,editTensorT = data2tensor(orgianlSenT, editedSenT)\n",
        "labelTensorT = torch.FloatTensor(testGrade_lst)\n",
        "orgTensorT,editTensorT=orgTensorT.type(torch.LongTensor),editTensorT.type(torch.LongTensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_HFQ_5rOPg9"
      },
      "source": [
        "## mini batch generating"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2MrY8Kvt4JC"
      },
      "source": [
        "class twoInDataset(tud.Dataset):\n",
        "    def __init__(self, x1, x2, y1):\n",
        "        self.len = x1.shape[0]\n",
        "        self.x1_data = x1.to(device)\n",
        "        self.x2_data = x2.to(device)\n",
        "        self.y1_data = y1.to(device)\n",
        "    def __getitem__(self, index):\n",
        "        return self.x1_data[index], self.x2_data[index], self.y1_data[index]\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "# Batching\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_dataset = twoInDataset(orgTensor, editTensor, labelTensor)\n",
        "valid_dataset = twoInDataset(orgTensorV, editTensorV, labelTensorV)\n",
        "test_dataset = twoInDataset(orgTensorT, editTensorT, labelTensorT)\n",
        "\n",
        "train_dataloader = tud.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "valid_dataloader = tud.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_dataloader = tud.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjPLZhlioGfk"
      },
      "source": [
        "# 1 input model dataset\n",
        "class oneInpDataset(tud.Dataset):\n",
        "    def __init__(self, x1, y1):\n",
        "        self.len = x1.shape[0]\n",
        "        self.x1_data = x1.to(device)\n",
        "        self.y1_data = y1.to(device)\n",
        "    def __getitem__(self, index):\n",
        "        return self.x1_data[index], self.y1_data[index]\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "\n",
        "#only study original sentences\n",
        "train_dataset_1Org = oneInpDataset(orgTensor, labelTensor)\n",
        "valid_dataset_1Org = oneInpDataset(orgTensorV, labelTensorV)\n",
        "\n",
        "train_dataloader_1Org = tud.DataLoader(train_dataset_1Org, batch_size=BATCH_SIZE, shuffle=True)\n",
        "valid_dataloader_1Org = tud.DataLoader(valid_dataset_1Org, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "#only study edited sentences\n",
        "train_dataset_1Edt = oneInpDataset(editTensor, labelTensor)\n",
        "valid_dataset_1Edt = oneInpDataset(editTensorV, labelTensorV)\n",
        "\n",
        "train_dataloader_1Edt = tud.DataLoader(train_dataset_1Edt, batch_size=BATCH_SIZE, shuffle=True)\n",
        "valid_dataloader_1Edt = tud.DataLoader(valid_dataset_1Edt, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0tkggVdHNNX"
      },
      "source": [
        "# test dataset for 1 input\n",
        "class testDataset(tud.Dataset):\n",
        "    def __init__(self, x1):\n",
        "        self.len = x1.shape[0]\n",
        "        self.x1_data = x1.to(device)\n",
        "    def __getitem__(self, index):\n",
        "        return self.x1_data[index]\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "\n",
        "test_dataset_org = testDataset(orgTensorT)\n",
        "test_dataloader_org = tud.DataLoader(test_dataset_org, batch_size=BATCH_SIZE)\n",
        "\n",
        "test_dataset_edt = testDataset(editTensorT)\n",
        "test_dataloader_edt = tud.DataLoader(test_dataset_edt, batch_size=BATCH_SIZE)\n",
        "\n",
        "# test dataset for 2 inputs\n",
        "class testDataset2in(tud.Dataset):\n",
        "    def __init__(self, x1,x2):\n",
        "        self.len = x1.shape[0]\n",
        "        self.x1_data = x1.to(device)\n",
        "        self.x2_data = x2.to(device)\n",
        "    def __getitem__(self, index):\n",
        "        return self.x1_data[index], self.x2_data[index]\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "test_dataset_2in = testDataset2in(orgTensorT,editTensorT)\n",
        "test_dataloader_2in = tud.DataLoader(test_dataset_2in, batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmEhvzAEWHU2"
      },
      "source": [
        "## Training Modes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lur9ZnKQjxjb"
      },
      "source": [
        "### 2 Inputs CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhDqme45jLmG"
      },
      "source": [
        "class TwoInputsCNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, out_channels, window_size, fc_out_dim, dropout):\n",
        "        super(TwoInputsCNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.conv = nn.Conv2d(in_channels=1, out_channels=out_channels,\n",
        "                                      kernel_size=(window_size, embedding_dim))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(out_channels, fc_out_dim)\n",
        "          \n",
        "    def forward(self, x, y):\n",
        "        xEmbed = self.embedding(x).unsqueeze(1)\n",
        "        yEmbed= self.embedding(y).unsqueeze(1)\n",
        "\n",
        "        xFeatureMaps = self.conv(xEmbed).squeeze(3)\n",
        "        yFeatureMaps = self.conv(yEmbed).squeeze(3)\n",
        "       \n",
        "        xFeatureMaps = F.relu(xFeatureMaps)\n",
        "        yFeatureMaps = F.relu(xFeatureMaps)\n",
        "        \n",
        "        xPool = F.max_pool1d(xFeatureMaps, xFeatureMaps.shape[2]).squeeze(2)\n",
        "        yPool = F.max_pool1d(yFeatureMaps, yFeatureMaps.shape[2]).squeeze(2)\n",
        "\n",
        "        xDrop = self.dropout(xPool)\n",
        "        yDrop = self.dropout(yPool)\n",
        "\n",
        "        out = xDrop * yDrop \n",
        "        preds = torch.sum(out, 1, keepdim = True)\n",
        "\n",
        "\n",
        "        return preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0BWQZdzfq4A"
      },
      "source": [
        "### 1 input CNN\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-laCRDcRfutd"
      },
      "source": [
        "class oneInputCNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, out_channels, window_size, fc_out_dim, dropout):\n",
        "        super(oneInputCNN, self).__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        \n",
        "        self.conv = nn.Conv2d(\n",
        "          in_channels=1, out_channels=out_channels,\n",
        "          kernel_size=(window_size, embedding_dim))\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.fc = nn.Linear(out_channels, fc_out_dim)\n",
        "          \n",
        "    def forward(self, x):\n",
        "        xEmbed = self.embedding(x).unsqueeze(1)\n",
        "\n",
        "        xFeatureMaps = self.conv(xEmbed).squeeze(3)\n",
        "\n",
        "        xFeatureMaps = F.relu(xFeatureMaps)\n",
        "        \n",
        "        xPool = F.max_pool1d(xFeatureMaps, xFeatureMaps.shape[2]).squeeze(2)\n",
        "\n",
        "        xDrop = self.dropout(xPool)\n",
        "\n",
        "        out = xDrop\n",
        "        preds = torch.sum(out, 1, keepdim = True)\n",
        "\n",
        "        return preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpBBR8PXZwgy"
      },
      "source": [
        "### 1 input BiLSTM (from approach 1 baseline)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCZqTfEzVH8I"
      },
      "source": [
        "class BiLSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, batch_size, device):\n",
        "        super(BiLSTM, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.device = device\n",
        "        self.batch_size = batch_size\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "\n",
        "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
        "        # with dimensionality hidden_dim.\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
        "\n",
        "        # The linear layer that maps from hidden state space to tag space\n",
        "        self.hidden2label = nn.Linear(hidden_dim * 2, 1)\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self):\n",
        "        # Before we've done anything, we dont have any hidden state.\n",
        "        # Refer to the Pytorch documentation to see exactly why they have this dimensionality.\n",
        "        # The axes semantics are (num_layers * num_directions, minibatch_size, hidden_dim)\n",
        "        return torch.zeros(2, self.batch_size, self.hidden_dim).to(self.device), \\\n",
        "               torch.zeros(2, self.batch_size, self.hidden_dim).to(self.device)\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embedded = self.embedding(sentence)\n",
        "        embedded = embedded.permute(1, 0, 2)\n",
        "\n",
        "        lstm_out, self.hidden = self.lstm(\n",
        "            embedded.view(len(embedded), self.batch_size, self.embedding_dim), self.hidden)\n",
        "\n",
        "        out = self.hidden2label(lstm_out[-1])\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJmg_bjyQlrR"
      },
      "source": [
        "### 2 inputs GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfK0eojTmdDu"
      },
      "source": [
        "class twoInputGRU(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, batch_size, device):\n",
        "        super(twoInputGRU, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.device = device\n",
        "        self.batch_size = batch_size\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, bidirectional=True)\n",
        "\n",
        "        self.hidden2label = nn.Linear(hidden_dim * 2, 1)\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return torch.zeros(2, self.batch_size, self.hidden_dim).to(self.device)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        embedded_x = self.embedding(x).permute(1, 0, 2)\n",
        "        embedded_y = self.embedding(y).permute(1, 0, 2)\n",
        "        gru_out1, self.hidden = self.gru(\n",
        "            embedded_x.view(len(embedded_x), self.batch_size, self.embedding_dim), self.hidden)\n",
        "        gru_out2, self.hidden = self.gru(\n",
        "            embedded_y.view(len(embedded_y), self.batch_size, self.embedding_dim), self.hidden)\n",
        "        out = self.hidden2label(gru_out1[-1]*gru_out2[-1])\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnaXAz61v4Nu"
      },
      "source": [
        "### 1 input GRU\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ti3s-roVv6fG"
      },
      "source": [
        "class GRU(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, batch_size, device):\n",
        "        super(GRU, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.device = device\n",
        "        self.batch_size = batch_size\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, bidirectional=True)\n",
        "\n",
        "        self.hidden2label = nn.Linear(hidden_dim * 2, 1)\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return torch.zeros(2, self.batch_size, self.hidden_dim).to(self.device)\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embedded = self.embedding(sentence)\n",
        "        embedded = embedded.permute(1, 0, 2)\n",
        "\n",
        "        gru_out, self.hidden = self.gru(\n",
        "            embedded.view(len(embedded), self.batch_size, self.embedding_dim), self.hidden)\n",
        "\n",
        "        out = self.hidden2label(gru_out[-1])\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-84nRP-mQsHw"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZoOfNPwDejv"
      },
      "source": [
        "### 2 inputs CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIls-slCiWLM",
        "outputId": "8e3a8fc7-12c5-4c60-cb64-3ab3f6fbb61e"
      },
      "source": [
        "# Set up hyperparameters\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "EPOCHS = 700\n",
        "LRATE = 0.001\n",
        "EMBEDDING_DIM = 50\n",
        "\n",
        "FC_OUT_DIM = 25\n",
        "N_OUT_CHAN = 100\n",
        "WINDOW_SIZE = 3\n",
        "DROPOUT = 0.7\n",
        "\n",
        "# Construct the model\n",
        "modelCNN = TwoInputsCNN(len(word2idx), EMBEDDING_DIM, N_OUT_CHAN, WINDOW_SIZE, FC_OUT_DIM, DROPOUT)\n",
        "\n",
        "modelCNN = modelCNN.to(device)\n",
        "\n",
        "optimizer = optim.Adam(modelCNN.parameters(), lr=LRATE)\n",
        "\n",
        "steps = 30 # set up the scheduler steps\n",
        "# use scheduler to dynamically change learning rate\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, steps)\n",
        "\n",
        "# prepare inputs and target output for both train and validation\n",
        "x_feature = orgTensor.to(device)\n",
        "y_feature = editTensor.to(device)\n",
        "target = labelTensor.to(device)\n",
        "\n",
        "valid_x_feature = orgTensorV.to(device)\n",
        "valid_y_feature = editTensorV.to(device)\n",
        "valid_target = labelTensorV.to(device)\n",
        "\n",
        "#train model through epoch\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    modelCNN.train()\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    predictions = modelCNN(x_feature, y_feature).squeeze(1)\n",
        "\n",
        "    loss = torch.sqrt(((predictions - target)**2).mean())\n",
        "    train_loss = loss.item()\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "      \n",
        "    #validation\n",
        "    modelCNN.eval()\n",
        "    with torch.no_grad():\n",
        "        valid_predictions = modelCNN(valid_x_feature, valid_y_feature).squeeze(1)\n",
        "        valid_loss = torch.sqrt(((valid_predictions - valid_target)**2).mean()).item()\n",
        "\n",
        "    print(f'| Epoch: {epoch:02} | Train rmse: {train_loss:.6f} | Val. rmse: {valid_loss:.6f} |')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Epoch: 01 | Train rmse: 101.400627 | Val. rmse: 90.121140 |\n",
            "| Epoch: 02 | Train rmse: 95.609360 | Val. rmse: 85.774170 |\n",
            "| Epoch: 03 | Train rmse: 91.666656 | Val. rmse: 81.683434 |\n",
            "| Epoch: 04 | Train rmse: 86.842010 | Val. rmse: 77.849220 |\n",
            "| Epoch: 05 | Train rmse: 83.879333 | Val. rmse: 74.272964 |\n",
            "| Epoch: 06 | Train rmse: 78.906982 | Val. rmse: 70.956192 |\n",
            "| Epoch: 07 | Train rmse: 75.558289 | Val. rmse: 67.890869 |\n",
            "| Epoch: 08 | Train rmse: 72.440826 | Val. rmse: 65.069473 |\n",
            "| Epoch: 09 | Train rmse: 69.150032 | Val. rmse: 62.486401 |\n",
            "| Epoch: 10 | Train rmse: 67.034012 | Val. rmse: 60.132481 |\n",
            "| Epoch: 11 | Train rmse: 63.825657 | Val. rmse: 57.998844 |\n",
            "| Epoch: 12 | Train rmse: 61.790863 | Val. rmse: 56.071560 |\n",
            "| Epoch: 13 | Train rmse: 59.683941 | Val. rmse: 54.339962 |\n",
            "| Epoch: 14 | Train rmse: 57.780762 | Val. rmse: 52.793133 |\n",
            "| Epoch: 15 | Train rmse: 56.362370 | Val. rmse: 51.420918 |\n",
            "| Epoch: 16 | Train rmse: 54.823067 | Val. rmse: 50.211590 |\n",
            "| Epoch: 17 | Train rmse: 52.853622 | Val. rmse: 49.155830 |\n",
            "| Epoch: 18 | Train rmse: 52.138981 | Val. rmse: 48.242722 |\n",
            "| Epoch: 19 | Train rmse: 51.364491 | Val. rmse: 47.461563 |\n",
            "| Epoch: 20 | Train rmse: 50.508652 | Val. rmse: 46.802349 |\n",
            "| Epoch: 21 | Train rmse: 49.967960 | Val. rmse: 46.255005 |\n",
            "| Epoch: 22 | Train rmse: 48.916084 | Val. rmse: 45.809795 |\n",
            "| Epoch: 23 | Train rmse: 48.567390 | Val. rmse: 45.456272 |\n",
            "| Epoch: 24 | Train rmse: 48.424103 | Val. rmse: 45.184196 |\n",
            "| Epoch: 25 | Train rmse: 47.914967 | Val. rmse: 44.983437 |\n",
            "| Epoch: 26 | Train rmse: 47.657059 | Val. rmse: 44.843472 |\n",
            "| Epoch: 27 | Train rmse: 47.834152 | Val. rmse: 44.753532 |\n",
            "| Epoch: 28 | Train rmse: 47.204121 | Val. rmse: 44.702812 |\n",
            "| Epoch: 29 | Train rmse: 47.764545 | Val. rmse: 44.680206 |\n",
            "| Epoch: 30 | Train rmse: 47.248714 | Val. rmse: 44.674549 |\n",
            "| Epoch: 31 | Train rmse: 47.226097 | Val. rmse: 44.674549 |\n",
            "| Epoch: 32 | Train rmse: 47.328140 | Val. rmse: 44.668907 |\n",
            "| Epoch: 33 | Train rmse: 47.670593 | Val. rmse: 44.646412 |\n",
            "| Epoch: 34 | Train rmse: 47.513721 | Val. rmse: 44.596062 |\n",
            "| Epoch: 35 | Train rmse: 47.173618 | Val. rmse: 44.507229 |\n",
            "| Epoch: 36 | Train rmse: 47.378670 | Val. rmse: 44.369755 |\n",
            "| Epoch: 37 | Train rmse: 47.533108 | Val. rmse: 44.174088 |\n",
            "| Epoch: 38 | Train rmse: 46.651714 | Val. rmse: 43.911896 |\n",
            "| Epoch: 39 | Train rmse: 47.035126 | Val. rmse: 43.575478 |\n",
            "| Epoch: 40 | Train rmse: 46.088402 | Val. rmse: 43.159031 |\n",
            "| Epoch: 41 | Train rmse: 45.912186 | Val. rmse: 42.657707 |\n",
            "| Epoch: 42 | Train rmse: 45.530254 | Val. rmse: 42.068272 |\n",
            "| Epoch: 43 | Train rmse: 44.570301 | Val. rmse: 41.389870 |\n",
            "| Epoch: 44 | Train rmse: 44.077763 | Val. rmse: 40.622494 |\n",
            "| Epoch: 45 | Train rmse: 43.398571 | Val. rmse: 39.768398 |\n",
            "| Epoch: 46 | Train rmse: 42.304913 | Val. rmse: 38.831779 |\n",
            "| Epoch: 47 | Train rmse: 41.463783 | Val. rmse: 37.818077 |\n",
            "| Epoch: 48 | Train rmse: 40.099777 | Val. rmse: 36.735081 |\n",
            "| Epoch: 49 | Train rmse: 38.772842 | Val. rmse: 35.592163 |\n",
            "| Epoch: 50 | Train rmse: 37.883442 | Val. rmse: 34.398113 |\n",
            "| Epoch: 51 | Train rmse: 36.278740 | Val. rmse: 33.164185 |\n",
            "| Epoch: 52 | Train rmse: 35.067341 | Val. rmse: 31.901474 |\n",
            "| Epoch: 53 | Train rmse: 33.844982 | Val. rmse: 30.620277 |\n",
            "| Epoch: 54 | Train rmse: 32.495716 | Val. rmse: 29.331383 |\n",
            "| Epoch: 55 | Train rmse: 31.121506 | Val. rmse: 28.045650 |\n",
            "| Epoch: 56 | Train rmse: 29.761086 | Val. rmse: 26.773474 |\n",
            "| Epoch: 57 | Train rmse: 28.283920 | Val. rmse: 25.524393 |\n",
            "| Epoch: 58 | Train rmse: 26.907841 | Val. rmse: 24.306789 |\n",
            "| Epoch: 59 | Train rmse: 25.621515 | Val. rmse: 23.128372 |\n",
            "| Epoch: 60 | Train rmse: 24.439596 | Val. rmse: 21.995380 |\n",
            "| Epoch: 61 | Train rmse: 23.369587 | Val. rmse: 20.912312 |\n",
            "| Epoch: 62 | Train rmse: 22.194078 | Val. rmse: 19.883142 |\n",
            "| Epoch: 63 | Train rmse: 21.117367 | Val. rmse: 18.910854 |\n",
            "| Epoch: 64 | Train rmse: 20.183405 | Val. rmse: 17.996492 |\n",
            "| Epoch: 65 | Train rmse: 19.090822 | Val. rmse: 17.141804 |\n",
            "| Epoch: 66 | Train rmse: 18.160900 | Val. rmse: 16.346731 |\n",
            "| Epoch: 67 | Train rmse: 17.126270 | Val. rmse: 15.611600 |\n",
            "| Epoch: 68 | Train rmse: 16.352409 | Val. rmse: 14.935247 |\n",
            "| Epoch: 69 | Train rmse: 15.749076 | Val. rmse: 14.315780 |\n",
            "| Epoch: 70 | Train rmse: 15.015635 | Val. rmse: 13.751380 |\n",
            "| Epoch: 71 | Train rmse: 14.359659 | Val. rmse: 13.239999 |\n",
            "| Epoch: 72 | Train rmse: 14.094242 | Val. rmse: 12.778692 |\n",
            "| Epoch: 73 | Train rmse: 13.432284 | Val. rmse: 12.365394 |\n",
            "| Epoch: 74 | Train rmse: 12.978143 | Val. rmse: 11.997326 |\n",
            "| Epoch: 75 | Train rmse: 12.644194 | Val. rmse: 11.671771 |\n",
            "| Epoch: 76 | Train rmse: 12.367437 | Val. rmse: 11.385775 |\n",
            "| Epoch: 77 | Train rmse: 11.861465 | Val. rmse: 11.137142 |\n",
            "| Epoch: 78 | Train rmse: 11.854097 | Val. rmse: 10.922836 |\n",
            "| Epoch: 79 | Train rmse: 11.703431 | Val. rmse: 10.740151 |\n",
            "| Epoch: 80 | Train rmse: 11.421863 | Val. rmse: 10.586599 |\n",
            "| Epoch: 81 | Train rmse: 11.180679 | Val. rmse: 10.459684 |\n",
            "| Epoch: 82 | Train rmse: 11.151153 | Val. rmse: 10.356751 |\n",
            "| Epoch: 83 | Train rmse: 11.036441 | Val. rmse: 10.275332 |\n",
            "| Epoch: 84 | Train rmse: 10.789429 | Val. rmse: 10.212977 |\n",
            "| Epoch: 85 | Train rmse: 10.771100 | Val. rmse: 10.167147 |\n",
            "| Epoch: 86 | Train rmse: 10.777424 | Val. rmse: 10.135292 |\n",
            "| Epoch: 87 | Train rmse: 10.767562 | Val. rmse: 10.114891 |\n",
            "| Epoch: 88 | Train rmse: 10.738977 | Val. rmse: 10.103414 |\n",
            "| Epoch: 89 | Train rmse: 10.710084 | Val. rmse: 10.098319 |\n",
            "| Epoch: 90 | Train rmse: 10.610173 | Val. rmse: 10.097047 |\n",
            "| Epoch: 91 | Train rmse: 10.713170 | Val. rmse: 10.097047 |\n",
            "| Epoch: 92 | Train rmse: 10.823132 | Val. rmse: 10.095784 |\n",
            "| Epoch: 93 | Train rmse: 10.544040 | Val. rmse: 10.090758 |\n",
            "| Epoch: 94 | Train rmse: 10.697707 | Val. rmse: 10.079525 |\n",
            "| Epoch: 95 | Train rmse: 10.724766 | Val. rmse: 10.059715 |\n",
            "| Epoch: 96 | Train rmse: 10.669660 | Val. rmse: 10.029078 |\n",
            "| Epoch: 97 | Train rmse: 10.675826 | Val. rmse: 9.985520 |\n",
            "| Epoch: 98 | Train rmse: 10.494875 | Val. rmse: 9.927161 |\n",
            "| Epoch: 99 | Train rmse: 10.549946 | Val. rmse: 9.852267 |\n",
            "| Epoch: 100 | Train rmse: 10.425853 | Val. rmse: 9.759440 |\n",
            "| Epoch: 101 | Train rmse: 10.323380 | Val. rmse: 9.647602 |\n",
            "| Epoch: 102 | Train rmse: 10.171413 | Val. rmse: 9.516014 |\n",
            "| Epoch: 103 | Train rmse: 10.053709 | Val. rmse: 9.364207 |\n",
            "| Epoch: 104 | Train rmse: 9.815111 | Val. rmse: 9.192270 |\n",
            "| Epoch: 105 | Train rmse: 9.864667 | Val. rmse: 9.000296 |\n",
            "| Epoch: 106 | Train rmse: 9.401713 | Val. rmse: 8.789461 |\n",
            "| Epoch: 107 | Train rmse: 9.281563 | Val. rmse: 8.560624 |\n",
            "| Epoch: 108 | Train rmse: 9.156203 | Val. rmse: 8.315061 |\n",
            "| Epoch: 109 | Train rmse: 8.825917 | Val. rmse: 8.054603 |\n",
            "| Epoch: 110 | Train rmse: 8.375698 | Val. rmse: 7.781744 |\n",
            "| Epoch: 111 | Train rmse: 8.192391 | Val. rmse: 7.498507 |\n",
            "| Epoch: 112 | Train rmse: 7.880804 | Val. rmse: 7.207321 |\n",
            "| Epoch: 113 | Train rmse: 7.663676 | Val. rmse: 6.910486 |\n",
            "| Epoch: 114 | Train rmse: 7.415978 | Val. rmse: 6.610325 |\n",
            "| Epoch: 115 | Train rmse: 6.951723 | Val. rmse: 6.309783 |\n",
            "| Epoch: 116 | Train rmse: 6.726754 | Val. rmse: 6.010917 |\n",
            "| Epoch: 117 | Train rmse: 6.325777 | Val. rmse: 5.716425 |\n",
            "| Epoch: 118 | Train rmse: 6.108524 | Val. rmse: 5.428037 |\n",
            "| Epoch: 119 | Train rmse: 5.755856 | Val. rmse: 5.147815 |\n",
            "| Epoch: 120 | Train rmse: 5.461844 | Val. rmse: 4.877524 |\n",
            "| Epoch: 121 | Train rmse: 5.194921 | Val. rmse: 4.618339 |\n",
            "| Epoch: 122 | Train rmse: 4.905435 | Val. rmse: 4.371605 |\n",
            "| Epoch: 123 | Train rmse: 4.653880 | Val. rmse: 4.138097 |\n",
            "| Epoch: 124 | Train rmse: 4.396627 | Val. rmse: 3.918472 |\n",
            "| Epoch: 125 | Train rmse: 4.107467 | Val. rmse: 3.713160 |\n",
            "| Epoch: 126 | Train rmse: 3.925844 | Val. rmse: 3.522350 |\n",
            "| Epoch: 127 | Train rmse: 3.818188 | Val. rmse: 3.345721 |\n",
            "| Epoch: 128 | Train rmse: 3.592029 | Val. rmse: 3.183286 |\n",
            "| Epoch: 129 | Train rmse: 3.407956 | Val. rmse: 3.034814 |\n",
            "| Epoch: 130 | Train rmse: 3.221128 | Val. rmse: 2.899924 |\n",
            "| Epoch: 131 | Train rmse: 3.128355 | Val. rmse: 2.777978 |\n",
            "| Epoch: 132 | Train rmse: 2.980656 | Val. rmse: 2.668431 |\n",
            "| Epoch: 133 | Train rmse: 2.883002 | Val. rmse: 2.570580 |\n",
            "| Epoch: 134 | Train rmse: 2.751333 | Val. rmse: 2.483782 |\n",
            "| Epoch: 135 | Train rmse: 2.661742 | Val. rmse: 2.407292 |\n",
            "| Epoch: 136 | Train rmse: 2.567379 | Val. rmse: 2.340500 |\n",
            "| Epoch: 137 | Train rmse: 2.530794 | Val. rmse: 2.282636 |\n",
            "| Epoch: 138 | Train rmse: 2.473909 | Val. rmse: 2.233017 |\n",
            "| Epoch: 139 | Train rmse: 2.435429 | Val. rmse: 2.190914 |\n",
            "| Epoch: 140 | Train rmse: 2.386480 | Val. rmse: 2.155683 |\n",
            "| Epoch: 141 | Train rmse: 2.356454 | Val. rmse: 2.126654 |\n",
            "| Epoch: 142 | Train rmse: 2.343443 | Val. rmse: 2.103212 |\n",
            "| Epoch: 143 | Train rmse: 2.295589 | Val. rmse: 2.084757 |\n",
            "| Epoch: 144 | Train rmse: 2.289208 | Val. rmse: 2.070656 |\n",
            "| Epoch: 145 | Train rmse: 2.281998 | Val. rmse: 2.060307 |\n",
            "| Epoch: 146 | Train rmse: 2.233487 | Val. rmse: 2.053136 |\n",
            "| Epoch: 147 | Train rmse: 2.235291 | Val. rmse: 2.048557 |\n",
            "| Epoch: 148 | Train rmse: 2.232585 | Val. rmse: 2.045986 |\n",
            "| Epoch: 149 | Train rmse: 2.247975 | Val. rmse: 2.044847 |\n",
            "| Epoch: 150 | Train rmse: 2.239810 | Val. rmse: 2.044563 |\n",
            "| Epoch: 151 | Train rmse: 2.234831 | Val. rmse: 2.044563 |\n",
            "| Epoch: 152 | Train rmse: 2.275661 | Val. rmse: 2.044281 |\n",
            "| Epoch: 153 | Train rmse: 2.218184 | Val. rmse: 2.043164 |\n",
            "| Epoch: 154 | Train rmse: 2.229843 | Val. rmse: 2.040672 |\n",
            "| Epoch: 155 | Train rmse: 2.218841 | Val. rmse: 2.036288 |\n",
            "| Epoch: 156 | Train rmse: 2.230593 | Val. rmse: 2.029520 |\n",
            "| Epoch: 157 | Train rmse: 2.210869 | Val. rmse: 2.019921 |\n",
            "| Epoch: 158 | Train rmse: 2.191247 | Val. rmse: 2.007087 |\n",
            "| Epoch: 159 | Train rmse: 2.179751 | Val. rmse: 1.990668 |\n",
            "| Epoch: 160 | Train rmse: 2.172095 | Val. rmse: 1.970359 |\n",
            "| Epoch: 161 | Train rmse: 2.159867 | Val. rmse: 1.945936 |\n",
            "| Epoch: 162 | Train rmse: 2.121357 | Val. rmse: 1.917247 |\n",
            "| Epoch: 163 | Train rmse: 2.074212 | Val. rmse: 1.884275 |\n",
            "| Epoch: 164 | Train rmse: 2.047238 | Val. rmse: 1.847043 |\n",
            "| Epoch: 165 | Train rmse: 2.031074 | Val. rmse: 1.805644 |\n",
            "| Epoch: 166 | Train rmse: 1.959797 | Val. rmse: 1.760358 |\n",
            "| Epoch: 167 | Train rmse: 1.934432 | Val. rmse: 1.711484 |\n",
            "| Epoch: 168 | Train rmse: 1.870897 | Val. rmse: 1.659422 |\n",
            "| Epoch: 169 | Train rmse: 1.846133 | Val. rmse: 1.604586 |\n",
            "| Epoch: 170 | Train rmse: 1.803212 | Val. rmse: 1.547461 |\n",
            "| Epoch: 171 | Train rmse: 1.713099 | Val. rmse: 1.488768 |\n",
            "| Epoch: 172 | Train rmse: 1.676006 | Val. rmse: 1.429086 |\n",
            "| Epoch: 173 | Train rmse: 1.591678 | Val. rmse: 1.369094 |\n",
            "| Epoch: 174 | Train rmse: 1.519704 | Val. rmse: 1.309484 |\n",
            "| Epoch: 175 | Train rmse: 1.489017 | Val. rmse: 1.250829 |\n",
            "| Epoch: 176 | Train rmse: 1.422612 | Val. rmse: 1.193826 |\n",
            "| Epoch: 177 | Train rmse: 1.342996 | Val. rmse: 1.139060 |\n",
            "| Epoch: 178 | Train rmse: 1.302594 | Val. rmse: 1.087052 |\n",
            "| Epoch: 179 | Train rmse: 1.231893 | Val. rmse: 1.038268 |\n",
            "| Epoch: 180 | Train rmse: 1.181859 | Val. rmse: 0.992986 |\n",
            "| Epoch: 181 | Train rmse: 1.132477 | Val. rmse: 0.951531 |\n",
            "| Epoch: 182 | Train rmse: 1.100170 | Val. rmse: 0.914021 |\n",
            "| Epoch: 183 | Train rmse: 1.051284 | Val. rmse: 0.880487 |\n",
            "| Epoch: 184 | Train rmse: 1.022470 | Val. rmse: 0.850865 |\n",
            "| Epoch: 185 | Train rmse: 0.984075 | Val. rmse: 0.825049 |\n",
            "| Epoch: 186 | Train rmse: 0.956692 | Val. rmse: 0.802833 |\n",
            "| Epoch: 187 | Train rmse: 0.943374 | Val. rmse: 0.783875 |\n",
            "| Epoch: 188 | Train rmse: 0.923912 | Val. rmse: 0.767826 |\n",
            "| Epoch: 189 | Train rmse: 0.904561 | Val. rmse: 0.754368 |\n",
            "| Epoch: 190 | Train rmse: 0.871444 | Val. rmse: 0.743204 |\n",
            "| Epoch: 191 | Train rmse: 0.860108 | Val. rmse: 0.733989 |\n",
            "| Epoch: 192 | Train rmse: 0.872323 | Val. rmse: 0.726393 |\n",
            "| Epoch: 193 | Train rmse: 0.847640 | Val. rmse: 0.720165 |\n",
            "| Epoch: 194 | Train rmse: 0.827548 | Val. rmse: 0.715083 |\n",
            "| Epoch: 195 | Train rmse: 0.825309 | Val. rmse: 0.710944 |\n",
            "| Epoch: 196 | Train rmse: 0.827962 | Val. rmse: 0.707579 |\n",
            "| Epoch: 197 | Train rmse: 0.823810 | Val. rmse: 0.704853 |\n",
            "| Epoch: 198 | Train rmse: 0.815550 | Val. rmse: 0.702654 |\n",
            "| Epoch: 199 | Train rmse: 0.810639 | Val. rmse: 0.700893 |\n",
            "| Epoch: 200 | Train rmse: 0.807246 | Val. rmse: 0.699489 |\n",
            "| Epoch: 201 | Train rmse: 0.805370 | Val. rmse: 0.698386 |\n",
            "| Epoch: 202 | Train rmse: 0.799861 | Val. rmse: 0.697528 |\n",
            "| Epoch: 203 | Train rmse: 0.806278 | Val. rmse: 0.696871 |\n",
            "| Epoch: 204 | Train rmse: 0.809945 | Val. rmse: 0.696382 |\n",
            "| Epoch: 205 | Train rmse: 0.803462 | Val. rmse: 0.696032 |\n",
            "| Epoch: 206 | Train rmse: 0.802573 | Val. rmse: 0.695792 |\n",
            "| Epoch: 207 | Train rmse: 0.795543 | Val. rmse: 0.695641 |\n",
            "| Epoch: 208 | Train rmse: 0.799750 | Val. rmse: 0.695558 |\n",
            "| Epoch: 209 | Train rmse: 0.804257 | Val. rmse: 0.695521 |\n",
            "| Epoch: 210 | Train rmse: 0.807298 | Val. rmse: 0.695512 |\n",
            "| Epoch: 211 | Train rmse: 0.798499 | Val. rmse: 0.695512 |\n",
            "| Epoch: 212 | Train rmse: 0.806364 | Val. rmse: 0.695504 |\n",
            "| Epoch: 213 | Train rmse: 0.805311 | Val. rmse: 0.695469 |\n",
            "| Epoch: 214 | Train rmse: 0.805215 | Val. rmse: 0.695392 |\n",
            "| Epoch: 215 | Train rmse: 0.796667 | Val. rmse: 0.695258 |\n",
            "| Epoch: 216 | Train rmse: 0.807237 | Val. rmse: 0.695054 |\n",
            "| Epoch: 217 | Train rmse: 0.801647 | Val. rmse: 0.694767 |\n",
            "| Epoch: 218 | Train rmse: 0.803614 | Val. rmse: 0.694386 |\n",
            "| Epoch: 219 | Train rmse: 0.801973 | Val. rmse: 0.693901 |\n",
            "| Epoch: 220 | Train rmse: 0.807878 | Val. rmse: 0.693304 |\n",
            "| Epoch: 221 | Train rmse: 0.799233 | Val. rmse: 0.692587 |\n",
            "| Epoch: 222 | Train rmse: 0.801275 | Val. rmse: 0.691752 |\n",
            "| Epoch: 223 | Train rmse: 0.799269 | Val. rmse: 0.690807 |\n",
            "| Epoch: 224 | Train rmse: 0.801258 | Val. rmse: 0.689752 |\n",
            "| Epoch: 225 | Train rmse: 0.789686 | Val. rmse: 0.688606 |\n",
            "| Epoch: 226 | Train rmse: 0.793728 | Val. rmse: 0.687377 |\n",
            "| Epoch: 227 | Train rmse: 0.790521 | Val. rmse: 0.686054 |\n",
            "| Epoch: 228 | Train rmse: 0.802176 | Val. rmse: 0.684636 |\n",
            "| Epoch: 229 | Train rmse: 0.778299 | Val. rmse: 0.683173 |\n",
            "| Epoch: 230 | Train rmse: 0.784322 | Val. rmse: 0.681674 |\n",
            "| Epoch: 231 | Train rmse: 0.782484 | Val. rmse: 0.680145 |\n",
            "| Epoch: 232 | Train rmse: 0.776690 | Val. rmse: 0.678598 |\n",
            "| Epoch: 233 | Train rmse: 0.774062 | Val. rmse: 0.677035 |\n",
            "| Epoch: 234 | Train rmse: 0.765589 | Val. rmse: 0.675469 |\n",
            "| Epoch: 235 | Train rmse: 0.775547 | Val. rmse: 0.673903 |\n",
            "| Epoch: 236 | Train rmse: 0.766021 | Val. rmse: 0.672358 |\n",
            "| Epoch: 237 | Train rmse: 0.773822 | Val. rmse: 0.670823 |\n",
            "| Epoch: 238 | Train rmse: 0.770041 | Val. rmse: 0.669309 |\n",
            "| Epoch: 239 | Train rmse: 0.760257 | Val. rmse: 0.667813 |\n",
            "| Epoch: 240 | Train rmse: 0.768772 | Val. rmse: 0.666347 |\n",
            "| Epoch: 241 | Train rmse: 0.760108 | Val. rmse: 0.664899 |\n",
            "| Epoch: 242 | Train rmse: 0.757646 | Val. rmse: 0.663482 |\n",
            "| Epoch: 243 | Train rmse: 0.748874 | Val. rmse: 0.662095 |\n",
            "| Epoch: 244 | Train rmse: 0.740068 | Val. rmse: 0.660745 |\n",
            "| Epoch: 245 | Train rmse: 0.745880 | Val. rmse: 0.659424 |\n",
            "| Epoch: 246 | Train rmse: 0.747633 | Val. rmse: 0.658147 |\n",
            "| Epoch: 247 | Train rmse: 0.746818 | Val. rmse: 0.656906 |\n",
            "| Epoch: 248 | Train rmse: 0.746512 | Val. rmse: 0.655729 |\n",
            "| Epoch: 249 | Train rmse: 0.738834 | Val. rmse: 0.654590 |\n",
            "| Epoch: 250 | Train rmse: 0.746914 | Val. rmse: 0.653510 |\n",
            "| Epoch: 251 | Train rmse: 0.740020 | Val. rmse: 0.652511 |\n",
            "| Epoch: 252 | Train rmse: 0.747087 | Val. rmse: 0.651597 |\n",
            "| Epoch: 253 | Train rmse: 0.740234 | Val. rmse: 0.650756 |\n",
            "| Epoch: 254 | Train rmse: 0.738483 | Val. rmse: 0.649996 |\n",
            "| Epoch: 255 | Train rmse: 0.730671 | Val. rmse: 0.649302 |\n",
            "| Epoch: 256 | Train rmse: 0.726388 | Val. rmse: 0.648685 |\n",
            "| Epoch: 257 | Train rmse: 0.723327 | Val. rmse: 0.648131 |\n",
            "| Epoch: 258 | Train rmse: 0.737176 | Val. rmse: 0.647648 |\n",
            "| Epoch: 259 | Train rmse: 0.737632 | Val. rmse: 0.647237 |\n",
            "| Epoch: 260 | Train rmse: 0.736746 | Val. rmse: 0.646889 |\n",
            "| Epoch: 261 | Train rmse: 0.722547 | Val. rmse: 0.646600 |\n",
            "| Epoch: 262 | Train rmse: 0.738637 | Val. rmse: 0.646367 |\n",
            "| Epoch: 263 | Train rmse: 0.729334 | Val. rmse: 0.646182 |\n",
            "| Epoch: 264 | Train rmse: 0.724865 | Val. rmse: 0.646037 |\n",
            "| Epoch: 265 | Train rmse: 0.734389 | Val. rmse: 0.645929 |\n",
            "| Epoch: 266 | Train rmse: 0.727502 | Val. rmse: 0.645857 |\n",
            "| Epoch: 267 | Train rmse: 0.731551 | Val. rmse: 0.645811 |\n",
            "| Epoch: 268 | Train rmse: 0.736482 | Val. rmse: 0.645785 |\n",
            "| Epoch: 269 | Train rmse: 0.727601 | Val. rmse: 0.645774 |\n",
            "| Epoch: 270 | Train rmse: 0.728190 | Val. rmse: 0.645771 |\n",
            "| Epoch: 271 | Train rmse: 0.728111 | Val. rmse: 0.645771 |\n",
            "| Epoch: 272 | Train rmse: 0.725349 | Val. rmse: 0.645768 |\n",
            "| Epoch: 273 | Train rmse: 0.735118 | Val. rmse: 0.645757 |\n",
            "| Epoch: 274 | Train rmse: 0.728167 | Val. rmse: 0.645732 |\n",
            "| Epoch: 275 | Train rmse: 0.724482 | Val. rmse: 0.645688 |\n",
            "| Epoch: 276 | Train rmse: 0.726115 | Val. rmse: 0.645620 |\n",
            "| Epoch: 277 | Train rmse: 0.734139 | Val. rmse: 0.645526 |\n",
            "| Epoch: 278 | Train rmse: 0.729930 | Val. rmse: 0.645401 |\n",
            "| Epoch: 279 | Train rmse: 0.726472 | Val. rmse: 0.645242 |\n",
            "| Epoch: 280 | Train rmse: 0.728450 | Val. rmse: 0.645043 |\n",
            "| Epoch: 281 | Train rmse: 0.727774 | Val. rmse: 0.644804 |\n",
            "| Epoch: 282 | Train rmse: 0.727245 | Val. rmse: 0.644523 |\n",
            "| Epoch: 283 | Train rmse: 0.721673 | Val. rmse: 0.644201 |\n",
            "| Epoch: 284 | Train rmse: 0.722785 | Val. rmse: 0.643830 |\n",
            "| Epoch: 285 | Train rmse: 0.729463 | Val. rmse: 0.643412 |\n",
            "| Epoch: 286 | Train rmse: 0.728410 | Val. rmse: 0.642932 |\n",
            "| Epoch: 287 | Train rmse: 0.721275 | Val. rmse: 0.642407 |\n",
            "| Epoch: 288 | Train rmse: 0.715827 | Val. rmse: 0.641848 |\n",
            "| Epoch: 289 | Train rmse: 0.719720 | Val. rmse: 0.641249 |\n",
            "| Epoch: 290 | Train rmse: 0.722847 | Val. rmse: 0.640597 |\n",
            "| Epoch: 291 | Train rmse: 0.731979 | Val. rmse: 0.639924 |\n",
            "| Epoch: 292 | Train rmse: 0.726539 | Val. rmse: 0.639206 |\n",
            "| Epoch: 293 | Train rmse: 0.724276 | Val. rmse: 0.638435 |\n",
            "| Epoch: 294 | Train rmse: 0.725636 | Val. rmse: 0.637648 |\n",
            "| Epoch: 295 | Train rmse: 0.712868 | Val. rmse: 0.636832 |\n",
            "| Epoch: 296 | Train rmse: 0.714544 | Val. rmse: 0.635993 |\n",
            "| Epoch: 297 | Train rmse: 0.719144 | Val. rmse: 0.635130 |\n",
            "| Epoch: 298 | Train rmse: 0.725393 | Val. rmse: 0.634298 |\n",
            "| Epoch: 299 | Train rmse: 0.707877 | Val. rmse: 0.633491 |\n",
            "| Epoch: 300 | Train rmse: 0.711297 | Val. rmse: 0.632706 |\n",
            "| Epoch: 301 | Train rmse: 0.710300 | Val. rmse: 0.631948 |\n",
            "| Epoch: 302 | Train rmse: 0.719608 | Val. rmse: 0.631222 |\n",
            "| Epoch: 303 | Train rmse: 0.703205 | Val. rmse: 0.630547 |\n",
            "| Epoch: 304 | Train rmse: 0.705837 | Val. rmse: 0.629924 |\n",
            "| Epoch: 305 | Train rmse: 0.703356 | Val. rmse: 0.629341 |\n",
            "| Epoch: 306 | Train rmse: 0.704977 | Val. rmse: 0.628775 |\n",
            "| Epoch: 307 | Train rmse: 0.705979 | Val. rmse: 0.628259 |\n",
            "| Epoch: 308 | Train rmse: 0.707147 | Val. rmse: 0.627785 |\n",
            "| Epoch: 309 | Train rmse: 0.703911 | Val. rmse: 0.627367 |\n",
            "| Epoch: 310 | Train rmse: 0.705542 | Val. rmse: 0.626975 |\n",
            "| Epoch: 311 | Train rmse: 0.707185 | Val. rmse: 0.626647 |\n",
            "| Epoch: 312 | Train rmse: 0.699623 | Val. rmse: 0.626340 |\n",
            "| Epoch: 313 | Train rmse: 0.703630 | Val. rmse: 0.626062 |\n",
            "| Epoch: 314 | Train rmse: 0.707403 | Val. rmse: 0.625821 |\n",
            "| Epoch: 315 | Train rmse: 0.710558 | Val. rmse: 0.625615 |\n",
            "| Epoch: 316 | Train rmse: 0.700058 | Val. rmse: 0.625447 |\n",
            "| Epoch: 317 | Train rmse: 0.713740 | Val. rmse: 0.625314 |\n",
            "| Epoch: 318 | Train rmse: 0.696321 | Val. rmse: 0.625198 |\n",
            "| Epoch: 319 | Train rmse: 0.701249 | Val. rmse: 0.625099 |\n",
            "| Epoch: 320 | Train rmse: 0.701274 | Val. rmse: 0.625018 |\n",
            "| Epoch: 321 | Train rmse: 0.697690 | Val. rmse: 0.624958 |\n",
            "| Epoch: 322 | Train rmse: 0.702225 | Val. rmse: 0.624910 |\n",
            "| Epoch: 323 | Train rmse: 0.691420 | Val. rmse: 0.624869 |\n",
            "| Epoch: 324 | Train rmse: 0.696146 | Val. rmse: 0.624833 |\n",
            "| Epoch: 325 | Train rmse: 0.695335 | Val. rmse: 0.624807 |\n",
            "| Epoch: 326 | Train rmse: 0.708092 | Val. rmse: 0.624790 |\n",
            "| Epoch: 327 | Train rmse: 0.698690 | Val. rmse: 0.624779 |\n",
            "| Epoch: 328 | Train rmse: 0.697782 | Val. rmse: 0.624771 |\n",
            "| Epoch: 329 | Train rmse: 0.697716 | Val. rmse: 0.624768 |\n",
            "| Epoch: 330 | Train rmse: 0.700791 | Val. rmse: 0.624768 |\n",
            "| Epoch: 331 | Train rmse: 0.697536 | Val. rmse: 0.624768 |\n",
            "| Epoch: 332 | Train rmse: 0.692725 | Val. rmse: 0.624767 |\n",
            "| Epoch: 333 | Train rmse: 0.695476 | Val. rmse: 0.624764 |\n",
            "| Epoch: 334 | Train rmse: 0.701460 | Val. rmse: 0.624757 |\n",
            "| Epoch: 335 | Train rmse: 0.707555 | Val. rmse: 0.624745 |\n",
            "| Epoch: 336 | Train rmse: 0.699954 | Val. rmse: 0.624726 |\n",
            "| Epoch: 337 | Train rmse: 0.705689 | Val. rmse: 0.624700 |\n",
            "| Epoch: 338 | Train rmse: 0.696140 | Val. rmse: 0.624663 |\n",
            "| Epoch: 339 | Train rmse: 0.699310 | Val. rmse: 0.624620 |\n",
            "| Epoch: 340 | Train rmse: 0.690713 | Val. rmse: 0.624561 |\n",
            "| Epoch: 341 | Train rmse: 0.695177 | Val. rmse: 0.624491 |\n",
            "| Epoch: 342 | Train rmse: 0.698263 | Val. rmse: 0.624414 |\n",
            "| Epoch: 343 | Train rmse: 0.696921 | Val. rmse: 0.624317 |\n",
            "| Epoch: 344 | Train rmse: 0.696886 | Val. rmse: 0.624213 |\n",
            "| Epoch: 345 | Train rmse: 0.686330 | Val. rmse: 0.624080 |\n",
            "| Epoch: 346 | Train rmse: 0.701284 | Val. rmse: 0.623939 |\n",
            "| Epoch: 347 | Train rmse: 0.691494 | Val. rmse: 0.623801 |\n",
            "| Epoch: 348 | Train rmse: 0.691415 | Val. rmse: 0.623658 |\n",
            "| Epoch: 349 | Train rmse: 0.691175 | Val. rmse: 0.623477 |\n",
            "| Epoch: 350 | Train rmse: 0.697555 | Val. rmse: 0.623287 |\n",
            "| Epoch: 351 | Train rmse: 0.697338 | Val. rmse: 0.623075 |\n",
            "| Epoch: 352 | Train rmse: 0.692020 | Val. rmse: 0.622844 |\n",
            "| Epoch: 353 | Train rmse: 0.696135 | Val. rmse: 0.622611 |\n",
            "| Epoch: 354 | Train rmse: 0.691284 | Val. rmse: 0.622339 |\n",
            "| Epoch: 355 | Train rmse: 0.692149 | Val. rmse: 0.622039 |\n",
            "| Epoch: 356 | Train rmse: 0.697299 | Val. rmse: 0.621751 |\n",
            "| Epoch: 357 | Train rmse: 0.686960 | Val. rmse: 0.621428 |\n",
            "| Epoch: 358 | Train rmse: 0.689700 | Val. rmse: 0.621098 |\n",
            "| Epoch: 359 | Train rmse: 0.687544 | Val. rmse: 0.620762 |\n",
            "| Epoch: 360 | Train rmse: 0.690519 | Val. rmse: 0.620433 |\n",
            "| Epoch: 361 | Train rmse: 0.686262 | Val. rmse: 0.620100 |\n",
            "| Epoch: 362 | Train rmse: 0.686923 | Val. rmse: 0.619769 |\n",
            "| Epoch: 363 | Train rmse: 0.685199 | Val. rmse: 0.619459 |\n",
            "| Epoch: 364 | Train rmse: 0.692030 | Val. rmse: 0.619162 |\n",
            "| Epoch: 365 | Train rmse: 0.684806 | Val. rmse: 0.618866 |\n",
            "| Epoch: 366 | Train rmse: 0.689305 | Val. rmse: 0.618598 |\n",
            "| Epoch: 367 | Train rmse: 0.683845 | Val. rmse: 0.618342 |\n",
            "| Epoch: 368 | Train rmse: 0.689254 | Val. rmse: 0.618149 |\n",
            "| Epoch: 369 | Train rmse: 0.681886 | Val. rmse: 0.617960 |\n",
            "| Epoch: 370 | Train rmse: 0.684002 | Val. rmse: 0.617769 |\n",
            "| Epoch: 371 | Train rmse: 0.681553 | Val. rmse: 0.617622 |\n",
            "| Epoch: 372 | Train rmse: 0.679004 | Val. rmse: 0.617504 |\n",
            "| Epoch: 373 | Train rmse: 0.683067 | Val. rmse: 0.617391 |\n",
            "| Epoch: 374 | Train rmse: 0.679281 | Val. rmse: 0.617282 |\n",
            "| Epoch: 375 | Train rmse: 0.682623 | Val. rmse: 0.617171 |\n",
            "| Epoch: 376 | Train rmse: 0.677062 | Val. rmse: 0.617060 |\n",
            "| Epoch: 377 | Train rmse: 0.677761 | Val. rmse: 0.616952 |\n",
            "| Epoch: 378 | Train rmse: 0.680008 | Val. rmse: 0.616848 |\n",
            "| Epoch: 379 | Train rmse: 0.685756 | Val. rmse: 0.616768 |\n",
            "| Epoch: 380 | Train rmse: 0.681605 | Val. rmse: 0.616707 |\n",
            "| Epoch: 381 | Train rmse: 0.674196 | Val. rmse: 0.616657 |\n",
            "| Epoch: 382 | Train rmse: 0.676424 | Val. rmse: 0.616613 |\n",
            "| Epoch: 383 | Train rmse: 0.673375 | Val. rmse: 0.616579 |\n",
            "| Epoch: 384 | Train rmse: 0.683741 | Val. rmse: 0.616553 |\n",
            "| Epoch: 385 | Train rmse: 0.681590 | Val. rmse: 0.616535 |\n",
            "| Epoch: 386 | Train rmse: 0.680037 | Val. rmse: 0.616524 |\n",
            "| Epoch: 387 | Train rmse: 0.682385 | Val. rmse: 0.616515 |\n",
            "| Epoch: 388 | Train rmse: 0.684205 | Val. rmse: 0.616511 |\n",
            "| Epoch: 389 | Train rmse: 0.674273 | Val. rmse: 0.616508 |\n",
            "| Epoch: 390 | Train rmse: 0.673533 | Val. rmse: 0.616508 |\n",
            "| Epoch: 391 | Train rmse: 0.676490 | Val. rmse: 0.616508 |\n",
            "| Epoch: 392 | Train rmse: 0.680496 | Val. rmse: 0.616508 |\n",
            "| Epoch: 393 | Train rmse: 0.675179 | Val. rmse: 0.616507 |\n",
            "| Epoch: 394 | Train rmse: 0.679012 | Val. rmse: 0.616504 |\n",
            "| Epoch: 395 | Train rmse: 0.684013 | Val. rmse: 0.616502 |\n",
            "| Epoch: 396 | Train rmse: 0.678535 | Val. rmse: 0.616496 |\n",
            "| Epoch: 397 | Train rmse: 0.672202 | Val. rmse: 0.616486 |\n",
            "| Epoch: 398 | Train rmse: 0.678705 | Val. rmse: 0.616478 |\n",
            "| Epoch: 399 | Train rmse: 0.680722 | Val. rmse: 0.616467 |\n",
            "| Epoch: 400 | Train rmse: 0.680325 | Val. rmse: 0.616457 |\n",
            "| Epoch: 401 | Train rmse: 0.687343 | Val. rmse: 0.616452 |\n",
            "| Epoch: 402 | Train rmse: 0.674103 | Val. rmse: 0.616436 |\n",
            "| Epoch: 403 | Train rmse: 0.683554 | Val. rmse: 0.616403 |\n",
            "| Epoch: 404 | Train rmse: 0.679265 | Val. rmse: 0.616361 |\n",
            "| Epoch: 405 | Train rmse: 0.679259 | Val. rmse: 0.616298 |\n",
            "| Epoch: 406 | Train rmse: 0.682961 | Val. rmse: 0.616226 |\n",
            "| Epoch: 407 | Train rmse: 0.688380 | Val. rmse: 0.616168 |\n",
            "| Epoch: 408 | Train rmse: 0.676764 | Val. rmse: 0.616079 |\n",
            "| Epoch: 409 | Train rmse: 0.681039 | Val. rmse: 0.615994 |\n",
            "| Epoch: 410 | Train rmse: 0.678257 | Val. rmse: 0.615889 |\n",
            "| Epoch: 411 | Train rmse: 0.677736 | Val. rmse: 0.615781 |\n",
            "| Epoch: 412 | Train rmse: 0.680978 | Val. rmse: 0.615722 |\n",
            "| Epoch: 413 | Train rmse: 0.679453 | Val. rmse: 0.615663 |\n",
            "| Epoch: 414 | Train rmse: 0.674841 | Val. rmse: 0.615564 |\n",
            "| Epoch: 415 | Train rmse: 0.674908 | Val. rmse: 0.615471 |\n",
            "| Epoch: 416 | Train rmse: 0.667931 | Val. rmse: 0.615357 |\n",
            "| Epoch: 417 | Train rmse: 0.679965 | Val. rmse: 0.615225 |\n",
            "| Epoch: 418 | Train rmse: 0.670546 | Val. rmse: 0.615094 |\n",
            "| Epoch: 419 | Train rmse: 0.673470 | Val. rmse: 0.614966 |\n",
            "| Epoch: 420 | Train rmse: 0.675509 | Val. rmse: 0.614794 |\n",
            "| Epoch: 421 | Train rmse: 0.674819 | Val. rmse: 0.614615 |\n",
            "| Epoch: 422 | Train rmse: 0.674595 | Val. rmse: 0.614442 |\n",
            "| Epoch: 423 | Train rmse: 0.671123 | Val. rmse: 0.614255 |\n",
            "| Epoch: 424 | Train rmse: 0.677484 | Val. rmse: 0.614079 |\n",
            "| Epoch: 425 | Train rmse: 0.671798 | Val. rmse: 0.613904 |\n",
            "| Epoch: 426 | Train rmse: 0.676467 | Val. rmse: 0.613797 |\n",
            "| Epoch: 427 | Train rmse: 0.662597 | Val. rmse: 0.613685 |\n",
            "| Epoch: 428 | Train rmse: 0.668565 | Val. rmse: 0.613556 |\n",
            "| Epoch: 429 | Train rmse: 0.673574 | Val. rmse: 0.613455 |\n",
            "| Epoch: 430 | Train rmse: 0.671780 | Val. rmse: 0.613351 |\n",
            "| Epoch: 431 | Train rmse: 0.668096 | Val. rmse: 0.613228 |\n",
            "| Epoch: 432 | Train rmse: 0.666215 | Val. rmse: 0.613105 |\n",
            "| Epoch: 433 | Train rmse: 0.667234 | Val. rmse: 0.612979 |\n",
            "| Epoch: 434 | Train rmse: 0.665533 | Val. rmse: 0.612834 |\n",
            "| Epoch: 435 | Train rmse: 0.669038 | Val. rmse: 0.612721 |\n",
            "| Epoch: 436 | Train rmse: 0.666455 | Val. rmse: 0.612617 |\n",
            "| Epoch: 437 | Train rmse: 0.663512 | Val. rmse: 0.612520 |\n",
            "| Epoch: 438 | Train rmse: 0.669635 | Val. rmse: 0.612445 |\n",
            "| Epoch: 439 | Train rmse: 0.648934 | Val. rmse: 0.612370 |\n",
            "| Epoch: 440 | Train rmse: 0.667544 | Val. rmse: 0.612309 |\n",
            "| Epoch: 441 | Train rmse: 0.671591 | Val. rmse: 0.612269 |\n",
            "| Epoch: 442 | Train rmse: 0.674989 | Val. rmse: 0.612245 |\n",
            "| Epoch: 443 | Train rmse: 0.668001 | Val. rmse: 0.612228 |\n",
            "| Epoch: 444 | Train rmse: 0.666228 | Val. rmse: 0.612210 |\n",
            "| Epoch: 445 | Train rmse: 0.665479 | Val. rmse: 0.612191 |\n",
            "| Epoch: 446 | Train rmse: 0.663106 | Val. rmse: 0.612178 |\n",
            "| Epoch: 447 | Train rmse: 0.662397 | Val. rmse: 0.612170 |\n",
            "| Epoch: 448 | Train rmse: 0.666764 | Val. rmse: 0.612166 |\n",
            "| Epoch: 449 | Train rmse: 0.665220 | Val. rmse: 0.612165 |\n",
            "| Epoch: 450 | Train rmse: 0.664744 | Val. rmse: 0.612165 |\n",
            "| Epoch: 451 | Train rmse: 0.662901 | Val. rmse: 0.612165 |\n",
            "| Epoch: 452 | Train rmse: 0.669648 | Val. rmse: 0.612165 |\n",
            "| Epoch: 453 | Train rmse: 0.667178 | Val. rmse: 0.612165 |\n",
            "| Epoch: 454 | Train rmse: 0.663491 | Val. rmse: 0.612164 |\n",
            "| Epoch: 455 | Train rmse: 0.664531 | Val. rmse: 0.612163 |\n",
            "| Epoch: 456 | Train rmse: 0.670357 | Val. rmse: 0.612162 |\n",
            "| Epoch: 457 | Train rmse: 0.664120 | Val. rmse: 0.612158 |\n",
            "| Epoch: 458 | Train rmse: 0.668833 | Val. rmse: 0.612154 |\n",
            "| Epoch: 459 | Train rmse: 0.669274 | Val. rmse: 0.612146 |\n",
            "| Epoch: 460 | Train rmse: 0.669087 | Val. rmse: 0.612136 |\n",
            "| Epoch: 461 | Train rmse: 0.662490 | Val. rmse: 0.612127 |\n",
            "| Epoch: 462 | Train rmse: 0.667935 | Val. rmse: 0.612116 |\n",
            "| Epoch: 463 | Train rmse: 0.661606 | Val. rmse: 0.612097 |\n",
            "| Epoch: 464 | Train rmse: 0.661645 | Val. rmse: 0.612073 |\n",
            "| Epoch: 465 | Train rmse: 0.666958 | Val. rmse: 0.612029 |\n",
            "| Epoch: 466 | Train rmse: 0.663569 | Val. rmse: 0.611984 |\n",
            "| Epoch: 467 | Train rmse: 0.665155 | Val. rmse: 0.611937 |\n",
            "| Epoch: 468 | Train rmse: 0.663033 | Val. rmse: 0.611904 |\n",
            "| Epoch: 469 | Train rmse: 0.659518 | Val. rmse: 0.611841 |\n",
            "| Epoch: 470 | Train rmse: 0.666997 | Val. rmse: 0.611768 |\n",
            "| Epoch: 471 | Train rmse: 0.667585 | Val. rmse: 0.611683 |\n",
            "| Epoch: 472 | Train rmse: 0.662863 | Val. rmse: 0.611565 |\n",
            "| Epoch: 473 | Train rmse: 0.667673 | Val. rmse: 0.611475 |\n",
            "| Epoch: 474 | Train rmse: 0.659541 | Val. rmse: 0.611387 |\n",
            "| Epoch: 475 | Train rmse: 0.665034 | Val. rmse: 0.611337 |\n",
            "| Epoch: 476 | Train rmse: 0.659990 | Val. rmse: 0.611262 |\n",
            "| Epoch: 477 | Train rmse: 0.665232 | Val. rmse: 0.611206 |\n",
            "| Epoch: 478 | Train rmse: 0.657220 | Val. rmse: 0.611154 |\n",
            "| Epoch: 479 | Train rmse: 0.655776 | Val. rmse: 0.611080 |\n",
            "| Epoch: 480 | Train rmse: 0.670857 | Val. rmse: 0.611025 |\n",
            "| Epoch: 481 | Train rmse: 0.660983 | Val. rmse: 0.611000 |\n",
            "| Epoch: 482 | Train rmse: 0.663746 | Val. rmse: 0.610964 |\n",
            "| Epoch: 483 | Train rmse: 0.661422 | Val. rmse: 0.610911 |\n",
            "| Epoch: 484 | Train rmse: 0.662004 | Val. rmse: 0.610844 |\n",
            "| Epoch: 485 | Train rmse: 0.663483 | Val. rmse: 0.610811 |\n",
            "| Epoch: 486 | Train rmse: 0.661441 | Val. rmse: 0.610740 |\n",
            "| Epoch: 487 | Train rmse: 0.657032 | Val. rmse: 0.610651 |\n",
            "| Epoch: 488 | Train rmse: 0.657227 | Val. rmse: 0.610561 |\n",
            "| Epoch: 489 | Train rmse: 0.652902 | Val. rmse: 0.610449 |\n",
            "| Epoch: 490 | Train rmse: 0.660781 | Val. rmse: 0.610366 |\n",
            "| Epoch: 491 | Train rmse: 0.668328 | Val. rmse: 0.610310 |\n",
            "| Epoch: 492 | Train rmse: 0.653341 | Val. rmse: 0.610241 |\n",
            "| Epoch: 493 | Train rmse: 0.661037 | Val. rmse: 0.610167 |\n",
            "| Epoch: 494 | Train rmse: 0.661250 | Val. rmse: 0.610084 |\n",
            "| Epoch: 495 | Train rmse: 0.654757 | Val. rmse: 0.609976 |\n",
            "| Epoch: 496 | Train rmse: 0.656396 | Val. rmse: 0.609861 |\n",
            "| Epoch: 497 | Train rmse: 0.661080 | Val. rmse: 0.609768 |\n",
            "| Epoch: 498 | Train rmse: 0.656154 | Val. rmse: 0.609663 |\n",
            "| Epoch: 499 | Train rmse: 0.647931 | Val. rmse: 0.609576 |\n",
            "| Epoch: 500 | Train rmse: 0.663846 | Val. rmse: 0.609506 |\n",
            "| Epoch: 501 | Train rmse: 0.656516 | Val. rmse: 0.609453 |\n",
            "| Epoch: 502 | Train rmse: 0.655553 | Val. rmse: 0.609411 |\n",
            "| Epoch: 503 | Train rmse: 0.659782 | Val. rmse: 0.609381 |\n",
            "| Epoch: 504 | Train rmse: 0.656020 | Val. rmse: 0.609360 |\n",
            "| Epoch: 505 | Train rmse: 0.653439 | Val. rmse: 0.609350 |\n",
            "| Epoch: 506 | Train rmse: 0.652464 | Val. rmse: 0.609347 |\n",
            "| Epoch: 507 | Train rmse: 0.659609 | Val. rmse: 0.609348 |\n",
            "| Epoch: 508 | Train rmse: 0.658100 | Val. rmse: 0.609349 |\n",
            "| Epoch: 509 | Train rmse: 0.661574 | Val. rmse: 0.609350 |\n",
            "| Epoch: 510 | Train rmse: 0.654390 | Val. rmse: 0.609350 |\n",
            "| Epoch: 511 | Train rmse: 0.663165 | Val. rmse: 0.609350 |\n",
            "| Epoch: 512 | Train rmse: 0.656810 | Val. rmse: 0.609350 |\n",
            "| Epoch: 513 | Train rmse: 0.651267 | Val. rmse: 0.609351 |\n",
            "| Epoch: 514 | Train rmse: 0.659390 | Val. rmse: 0.609355 |\n",
            "| Epoch: 515 | Train rmse: 0.654933 | Val. rmse: 0.609361 |\n",
            "| Epoch: 516 | Train rmse: 0.660908 | Val. rmse: 0.609369 |\n",
            "| Epoch: 517 | Train rmse: 0.659811 | Val. rmse: 0.609384 |\n",
            "| Epoch: 518 | Train rmse: 0.658259 | Val. rmse: 0.609402 |\n",
            "| Epoch: 519 | Train rmse: 0.651432 | Val. rmse: 0.609419 |\n",
            "| Epoch: 520 | Train rmse: 0.655527 | Val. rmse: 0.609448 |\n",
            "| Epoch: 521 | Train rmse: 0.658920 | Val. rmse: 0.609478 |\n",
            "| Epoch: 522 | Train rmse: 0.660610 | Val. rmse: 0.609503 |\n",
            "| Epoch: 523 | Train rmse: 0.651989 | Val. rmse: 0.609528 |\n",
            "| Epoch: 524 | Train rmse: 0.660047 | Val. rmse: 0.609563 |\n",
            "| Epoch: 525 | Train rmse: 0.656050 | Val. rmse: 0.609588 |\n",
            "| Epoch: 526 | Train rmse: 0.658289 | Val. rmse: 0.609616 |\n",
            "| Epoch: 527 | Train rmse: 0.655018 | Val. rmse: 0.609635 |\n",
            "| Epoch: 528 | Train rmse: 0.653099 | Val. rmse: 0.609646 |\n",
            "| Epoch: 529 | Train rmse: 0.656918 | Val. rmse: 0.609650 |\n",
            "| Epoch: 530 | Train rmse: 0.644867 | Val. rmse: 0.609630 |\n",
            "| Epoch: 531 | Train rmse: 0.661078 | Val. rmse: 0.609614 |\n",
            "| Epoch: 532 | Train rmse: 0.660324 | Val. rmse: 0.609581 |\n",
            "| Epoch: 533 | Train rmse: 0.654062 | Val. rmse: 0.609462 |\n",
            "| Epoch: 534 | Train rmse: 0.651342 | Val. rmse: 0.609326 |\n",
            "| Epoch: 535 | Train rmse: 0.651293 | Val. rmse: 0.609199 |\n",
            "| Epoch: 536 | Train rmse: 0.657347 | Val. rmse: 0.609027 |\n",
            "| Epoch: 537 | Train rmse: 0.649159 | Val. rmse: 0.608826 |\n",
            "| Epoch: 538 | Train rmse: 0.650902 | Val. rmse: 0.608609 |\n",
            "| Epoch: 539 | Train rmse: 0.657882 | Val. rmse: 0.608386 |\n",
            "| Epoch: 540 | Train rmse: 0.646903 | Val. rmse: 0.608133 |\n",
            "| Epoch: 541 | Train rmse: 0.654125 | Val. rmse: 0.607882 |\n",
            "| Epoch: 542 | Train rmse: 0.656094 | Val. rmse: 0.607662 |\n",
            "| Epoch: 543 | Train rmse: 0.652212 | Val. rmse: 0.607499 |\n",
            "| Epoch: 544 | Train rmse: 0.647538 | Val. rmse: 0.607349 |\n",
            "| Epoch: 545 | Train rmse: 0.651829 | Val. rmse: 0.607261 |\n",
            "| Epoch: 546 | Train rmse: 0.653919 | Val. rmse: 0.607227 |\n",
            "| Epoch: 547 | Train rmse: 0.650465 | Val. rmse: 0.607207 |\n",
            "| Epoch: 548 | Train rmse: 0.652761 | Val. rmse: 0.607220 |\n",
            "| Epoch: 549 | Train rmse: 0.654597 | Val. rmse: 0.607334 |\n",
            "| Epoch: 550 | Train rmse: 0.648855 | Val. rmse: 0.607433 |\n",
            "| Epoch: 551 | Train rmse: 0.650657 | Val. rmse: 0.607515 |\n",
            "| Epoch: 552 | Train rmse: 0.643597 | Val. rmse: 0.607588 |\n",
            "| Epoch: 553 | Train rmse: 0.649809 | Val. rmse: 0.607671 |\n",
            "| Epoch: 554 | Train rmse: 0.652283 | Val. rmse: 0.607741 |\n",
            "| Epoch: 555 | Train rmse: 0.653210 | Val. rmse: 0.607804 |\n",
            "| Epoch: 556 | Train rmse: 0.644195 | Val. rmse: 0.607853 |\n",
            "| Epoch: 557 | Train rmse: 0.646979 | Val. rmse: 0.607906 |\n",
            "| Epoch: 558 | Train rmse: 0.652897 | Val. rmse: 0.607962 |\n",
            "| Epoch: 559 | Train rmse: 0.650591 | Val. rmse: 0.608012 |\n",
            "| Epoch: 560 | Train rmse: 0.648811 | Val. rmse: 0.608043 |\n",
            "| Epoch: 561 | Train rmse: 0.658505 | Val. rmse: 0.608078 |\n",
            "| Epoch: 562 | Train rmse: 0.653289 | Val. rmse: 0.608116 |\n",
            "| Epoch: 563 | Train rmse: 0.645901 | Val. rmse: 0.608137 |\n",
            "| Epoch: 564 | Train rmse: 0.646825 | Val. rmse: 0.608149 |\n",
            "| Epoch: 565 | Train rmse: 0.645999 | Val. rmse: 0.608155 |\n",
            "| Epoch: 566 | Train rmse: 0.651792 | Val. rmse: 0.608157 |\n",
            "| Epoch: 567 | Train rmse: 0.646865 | Val. rmse: 0.608160 |\n",
            "| Epoch: 568 | Train rmse: 0.647975 | Val. rmse: 0.608162 |\n",
            "| Epoch: 569 | Train rmse: 0.653219 | Val. rmse: 0.608163 |\n",
            "| Epoch: 570 | Train rmse: 0.643056 | Val. rmse: 0.608163 |\n",
            "| Epoch: 571 | Train rmse: 0.656837 | Val. rmse: 0.608163 |\n",
            "| Epoch: 572 | Train rmse: 0.640131 | Val. rmse: 0.608163 |\n",
            "| Epoch: 573 | Train rmse: 0.647515 | Val. rmse: 0.608163 |\n",
            "| Epoch: 574 | Train rmse: 0.645071 | Val. rmse: 0.608162 |\n",
            "| Epoch: 575 | Train rmse: 0.644914 | Val. rmse: 0.608162 |\n",
            "| Epoch: 576 | Train rmse: 0.647977 | Val. rmse: 0.608159 |\n",
            "| Epoch: 577 | Train rmse: 0.654584 | Val. rmse: 0.608156 |\n",
            "| Epoch: 578 | Train rmse: 0.650920 | Val. rmse: 0.608151 |\n",
            "| Epoch: 579 | Train rmse: 0.648138 | Val. rmse: 0.608146 |\n",
            "| Epoch: 580 | Train rmse: 0.651101 | Val. rmse: 0.608129 |\n",
            "| Epoch: 581 | Train rmse: 0.649983 | Val. rmse: 0.608094 |\n",
            "| Epoch: 582 | Train rmse: 0.642032 | Val. rmse: 0.608041 |\n",
            "| Epoch: 583 | Train rmse: 0.651086 | Val. rmse: 0.607988 |\n",
            "| Epoch: 584 | Train rmse: 0.641878 | Val. rmse: 0.607929 |\n",
            "| Epoch: 585 | Train rmse: 0.650565 | Val. rmse: 0.607866 |\n",
            "| Epoch: 586 | Train rmse: 0.647347 | Val. rmse: 0.607793 |\n",
            "| Epoch: 587 | Train rmse: 0.645111 | Val. rmse: 0.607704 |\n",
            "| Epoch: 588 | Train rmse: 0.647756 | Val. rmse: 0.607616 |\n",
            "| Epoch: 589 | Train rmse: 0.648511 | Val. rmse: 0.607495 |\n",
            "| Epoch: 590 | Train rmse: 0.648153 | Val. rmse: 0.607339 |\n",
            "| Epoch: 591 | Train rmse: 0.645419 | Val. rmse: 0.607180 |\n",
            "| Epoch: 592 | Train rmse: 0.645005 | Val. rmse: 0.607080 |\n",
            "| Epoch: 593 | Train rmse: 0.643438 | Val. rmse: 0.606994 |\n",
            "| Epoch: 594 | Train rmse: 0.652578 | Val. rmse: 0.606941 |\n",
            "| Epoch: 595 | Train rmse: 0.644718 | Val. rmse: 0.606930 |\n",
            "| Epoch: 596 | Train rmse: 0.648906 | Val. rmse: 0.606960 |\n",
            "| Epoch: 597 | Train rmse: 0.645145 | Val. rmse: 0.606980 |\n",
            "| Epoch: 598 | Train rmse: 0.647644 | Val. rmse: 0.607006 |\n",
            "| Epoch: 599 | Train rmse: 0.635727 | Val. rmse: 0.607049 |\n",
            "| Epoch: 600 | Train rmse: 0.637560 | Val. rmse: 0.607074 |\n",
            "| Epoch: 601 | Train rmse: 0.643431 | Val. rmse: 0.607121 |\n",
            "| Epoch: 602 | Train rmse: 0.643143 | Val. rmse: 0.607225 |\n",
            "| Epoch: 603 | Train rmse: 0.638939 | Val. rmse: 0.607301 |\n",
            "| Epoch: 604 | Train rmse: 0.647087 | Val. rmse: 0.607294 |\n",
            "| Epoch: 605 | Train rmse: 0.646540 | Val. rmse: 0.607267 |\n",
            "| Epoch: 606 | Train rmse: 0.642513 | Val. rmse: 0.607213 |\n",
            "| Epoch: 607 | Train rmse: 0.642630 | Val. rmse: 0.607143 |\n",
            "| Epoch: 608 | Train rmse: 0.647905 | Val. rmse: 0.607111 |\n",
            "| Epoch: 609 | Train rmse: 0.644222 | Val. rmse: 0.607049 |\n",
            "| Epoch: 610 | Train rmse: 0.643121 | Val. rmse: 0.606983 |\n",
            "| Epoch: 611 | Train rmse: 0.638381 | Val. rmse: 0.606922 |\n",
            "| Epoch: 612 | Train rmse: 0.640118 | Val. rmse: 0.606875 |\n",
            "| Epoch: 613 | Train rmse: 0.640166 | Val. rmse: 0.606805 |\n",
            "| Epoch: 614 | Train rmse: 0.633553 | Val. rmse: 0.606733 |\n",
            "| Epoch: 615 | Train rmse: 0.641217 | Val. rmse: 0.606661 |\n",
            "| Epoch: 616 | Train rmse: 0.640549 | Val. rmse: 0.606590 |\n",
            "| Epoch: 617 | Train rmse: 0.642134 | Val. rmse: 0.606517 |\n",
            "| Epoch: 618 | Train rmse: 0.640384 | Val. rmse: 0.606470 |\n",
            "| Epoch: 619 | Train rmse: 0.643331 | Val. rmse: 0.606442 |\n",
            "| Epoch: 620 | Train rmse: 0.643732 | Val. rmse: 0.606421 |\n",
            "| Epoch: 621 | Train rmse: 0.642759 | Val. rmse: 0.606411 |\n",
            "| Epoch: 622 | Train rmse: 0.643957 | Val. rmse: 0.606407 |\n",
            "| Epoch: 623 | Train rmse: 0.641238 | Val. rmse: 0.606409 |\n",
            "| Epoch: 624 | Train rmse: 0.642180 | Val. rmse: 0.606422 |\n",
            "| Epoch: 625 | Train rmse: 0.640541 | Val. rmse: 0.606432 |\n",
            "| Epoch: 626 | Train rmse: 0.647888 | Val. rmse: 0.606443 |\n",
            "| Epoch: 627 | Train rmse: 0.644938 | Val. rmse: 0.606452 |\n",
            "| Epoch: 628 | Train rmse: 0.644834 | Val. rmse: 0.606458 |\n",
            "| Epoch: 629 | Train rmse: 0.646428 | Val. rmse: 0.606461 |\n",
            "| Epoch: 630 | Train rmse: 0.637965 | Val. rmse: 0.606462 |\n",
            "| Epoch: 631 | Train rmse: 0.643810 | Val. rmse: 0.606462 |\n",
            "| Epoch: 632 | Train rmse: 0.643875 | Val. rmse: 0.606463 |\n",
            "| Epoch: 633 | Train rmse: 0.638563 | Val. rmse: 0.606465 |\n",
            "| Epoch: 634 | Train rmse: 0.639391 | Val. rmse: 0.606471 |\n",
            "| Epoch: 635 | Train rmse: 0.640628 | Val. rmse: 0.606483 |\n",
            "| Epoch: 636 | Train rmse: 0.644588 | Val. rmse: 0.606502 |\n",
            "| Epoch: 637 | Train rmse: 0.638308 | Val. rmse: 0.606528 |\n",
            "| Epoch: 638 | Train rmse: 0.641304 | Val. rmse: 0.606561 |\n",
            "| Epoch: 639 | Train rmse: 0.637799 | Val. rmse: 0.606593 |\n",
            "| Epoch: 640 | Train rmse: 0.638491 | Val. rmse: 0.606626 |\n",
            "| Epoch: 641 | Train rmse: 0.639363 | Val. rmse: 0.606644 |\n",
            "| Epoch: 642 | Train rmse: 0.642832 | Val. rmse: 0.606680 |\n",
            "| Epoch: 643 | Train rmse: 0.650489 | Val. rmse: 0.606722 |\n",
            "| Epoch: 644 | Train rmse: 0.632562 | Val. rmse: 0.606723 |\n",
            "| Epoch: 645 | Train rmse: 0.636460 | Val. rmse: 0.606687 |\n",
            "| Epoch: 646 | Train rmse: 0.638951 | Val. rmse: 0.606627 |\n",
            "| Epoch: 647 | Train rmse: 0.642594 | Val. rmse: 0.606600 |\n",
            "| Epoch: 648 | Train rmse: 0.639926 | Val. rmse: 0.606596 |\n",
            "| Epoch: 649 | Train rmse: 0.641093 | Val. rmse: 0.606575 |\n",
            "| Epoch: 650 | Train rmse: 0.640875 | Val. rmse: 0.606554 |\n",
            "| Epoch: 651 | Train rmse: 0.642444 | Val. rmse: 0.606538 |\n",
            "| Epoch: 652 | Train rmse: 0.635007 | Val. rmse: 0.606480 |\n",
            "| Epoch: 653 | Train rmse: 0.640439 | Val. rmse: 0.606405 |\n",
            "| Epoch: 654 | Train rmse: 0.647212 | Val. rmse: 0.606344 |\n",
            "| Epoch: 655 | Train rmse: 0.642743 | Val. rmse: 0.606316 |\n",
            "| Epoch: 656 | Train rmse: 0.638281 | Val. rmse: 0.606291 |\n",
            "| Epoch: 657 | Train rmse: 0.636437 | Val. rmse: 0.606278 |\n",
            "| Epoch: 658 | Train rmse: 0.642315 | Val. rmse: 0.606248 |\n",
            "| Epoch: 659 | Train rmse: 0.641632 | Val. rmse: 0.606259 |\n",
            "| Epoch: 660 | Train rmse: 0.635729 | Val. rmse: 0.606337 |\n",
            "| Epoch: 661 | Train rmse: 0.645603 | Val. rmse: 0.606455 |\n",
            "| Epoch: 662 | Train rmse: 0.637640 | Val. rmse: 0.606564 |\n",
            "| Epoch: 663 | Train rmse: 0.644274 | Val. rmse: 0.606668 |\n",
            "| Epoch: 664 | Train rmse: 0.634497 | Val. rmse: 0.606688 |\n",
            "| Epoch: 665 | Train rmse: 0.638089 | Val. rmse: 0.606662 |\n",
            "| Epoch: 666 | Train rmse: 0.643392 | Val. rmse: 0.606646 |\n",
            "| Epoch: 667 | Train rmse: 0.634299 | Val. rmse: 0.606657 |\n",
            "| Epoch: 668 | Train rmse: 0.635896 | Val. rmse: 0.606645 |\n",
            "| Epoch: 669 | Train rmse: 0.636201 | Val. rmse: 0.606611 |\n",
            "| Epoch: 670 | Train rmse: 0.641399 | Val. rmse: 0.606620 |\n",
            "| Epoch: 671 | Train rmse: 0.634226 | Val. rmse: 0.606604 |\n",
            "| Epoch: 672 | Train rmse: 0.638123 | Val. rmse: 0.606569 |\n",
            "| Epoch: 673 | Train rmse: 0.640001 | Val. rmse: 0.606518 |\n",
            "| Epoch: 674 | Train rmse: 0.641073 | Val. rmse: 0.606499 |\n",
            "| Epoch: 675 | Train rmse: 0.634653 | Val. rmse: 0.606496 |\n",
            "| Epoch: 676 | Train rmse: 0.642075 | Val. rmse: 0.606477 |\n",
            "| Epoch: 677 | Train rmse: 0.638705 | Val. rmse: 0.606457 |\n",
            "| Epoch: 678 | Train rmse: 0.628973 | Val. rmse: 0.606434 |\n",
            "| Epoch: 679 | Train rmse: 0.639415 | Val. rmse: 0.606397 |\n",
            "| Epoch: 680 | Train rmse: 0.640558 | Val. rmse: 0.606368 |\n",
            "| Epoch: 681 | Train rmse: 0.641447 | Val. rmse: 0.606346 |\n",
            "| Epoch: 682 | Train rmse: 0.637196 | Val. rmse: 0.606338 |\n",
            "| Epoch: 683 | Train rmse: 0.636891 | Val. rmse: 0.606333 |\n",
            "| Epoch: 684 | Train rmse: 0.637039 | Val. rmse: 0.606323 |\n",
            "| Epoch: 685 | Train rmse: 0.643102 | Val. rmse: 0.606325 |\n",
            "| Epoch: 686 | Train rmse: 0.634726 | Val. rmse: 0.606325 |\n",
            "| Epoch: 687 | Train rmse: 0.636714 | Val. rmse: 0.606322 |\n",
            "| Epoch: 688 | Train rmse: 0.637754 | Val. rmse: 0.606321 |\n",
            "| Epoch: 689 | Train rmse: 0.640126 | Val. rmse: 0.606321 |\n",
            "| Epoch: 690 | Train rmse: 0.639556 | Val. rmse: 0.606321 |\n",
            "| Epoch: 691 | Train rmse: 0.630994 | Val. rmse: 0.606321 |\n",
            "| Epoch: 692 | Train rmse: 0.641590 | Val. rmse: 0.606321 |\n",
            "| Epoch: 693 | Train rmse: 0.644048 | Val. rmse: 0.606322 |\n",
            "| Epoch: 694 | Train rmse: 0.638444 | Val. rmse: 0.606322 |\n",
            "| Epoch: 695 | Train rmse: 0.633609 | Val. rmse: 0.606323 |\n",
            "| Epoch: 696 | Train rmse: 0.641180 | Val. rmse: 0.606328 |\n",
            "| Epoch: 697 | Train rmse: 0.637375 | Val. rmse: 0.606339 |\n",
            "| Epoch: 698 | Train rmse: 0.642218 | Val. rmse: 0.606356 |\n",
            "| Epoch: 699 | Train rmse: 0.631394 | Val. rmse: 0.606376 |\n",
            "| Epoch: 700 | Train rmse: 0.635149 | Val. rmse: 0.606401 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEVbNK4lTK3W",
        "outputId": "766be372-56a4-4578-cce9-e353744886d2"
      },
      "source": [
        "#####Check the test RMSE####\n",
        "modelCNN.eval()\n",
        "\n",
        "test_x_feature = orgTensorT.to(device)\n",
        "test_y_feature = editTensorT.to(device)\n",
        "test_target = labelTensorT.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_predictions = modelCNN(test_x_feature, test_y_feature).squeeze(1)\n",
        "    test_loss = torch.sqrt(((test_predictions - test_target)**2).mean()).item()\n",
        "\n",
        "print(f'train rmse: {train_loss:.6f} ; test. rmse: {test_loss:.6f} ')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train rmse: 0.635149 ; test. rmse: 0.611087 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_z7FK4khCiO"
      },
      "source": [
        "### 1 input CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKt3y1kkhE_T",
        "outputId": "bbba49fa-703b-4ab4-faa0-88d7c3a527c5"
      },
      "source": [
        "# Set up hyperparameters\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "EPOCHS = 300\n",
        "LRATE = 0.001\n",
        "\n",
        "EMBEDDING_DIM = 50\n",
        "FC_OUT_DIM = 25\n",
        "\n",
        "N_OUT_CHANNELS = 100\n",
        "\n",
        "WINDOW_SIZE = 3\n",
        "DROPOUT = 0.7\n",
        "\n",
        "# Construct the model\n",
        "modelCNN1 = oneInputCNN(len(word2idx), EMBEDDING_DIM, N_OUT_CHANNELS, WINDOW_SIZE, FC_OUT_DIM, DROPOUT)\n",
        "\n",
        "modelCNN1 = modelCNN1.to(device)\n",
        "\n",
        "optimizer = optim.Adam(modelCNN1.parameters(), lr=LRATE)\n",
        "\n",
        "#scheduler and its steps\n",
        "steps = 50\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, steps)\n",
        "\n",
        "#prepare train and validation dataset\n",
        "orgSen = orgTensor.to(device)\n",
        "edtSen = editTensor.to(device)\n",
        "target = labelTensor.to(device)\n",
        "\n",
        "valid_orgSen = orgTensorV.to(device)\n",
        "valid_target = labelTensorV.to(device)\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    modelCNN1.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    #original sentence as input\n",
        "    predictions = modelCNN1(orgSen).squeeze(1)\n",
        "\n",
        "    #edited sentence as input\n",
        "    #predictions = modelCNN1(edtSen).squeeze(1)\n",
        "\n",
        "    loss = torch.sqrt(((predictions - target)**2).mean())\n",
        "    train_loss = loss.item()\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    #validation\n",
        "    modelCNN1.eval()\n",
        "    with torch.no_grad():\n",
        "        valid_predictions = modelCNN1(valid_orgSen).squeeze(1)\n",
        "        valid_loss = torch.sqrt(((valid_predictions - valid_target)**2).mean()).item()\n",
        "\n",
        "    print(f'| Epoch: {epoch:02} | Train rmse: {train_loss:.6f} | Val. rmse: {valid_loss:.6f} |')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Epoch: 01 | Train rmse: 89.754951 | Val. rmse: 86.275620 |\n",
            "| Epoch: 02 | Train rmse: 87.111687 | Val. rmse: 83.990944 |\n",
            "| Epoch: 03 | Train rmse: 84.561600 | Val. rmse: 81.775063 |\n",
            "| Epoch: 04 | Train rmse: 82.587685 | Val. rmse: 79.628426 |\n",
            "| Epoch: 05 | Train rmse: 80.237961 | Val. rmse: 77.552330 |\n",
            "| Epoch: 06 | Train rmse: 78.214066 | Val. rmse: 75.546646 |\n",
            "| Epoch: 07 | Train rmse: 76.163582 | Val. rmse: 73.611198 |\n",
            "| Epoch: 08 | Train rmse: 74.180397 | Val. rmse: 71.748070 |\n",
            "| Epoch: 09 | Train rmse: 72.204643 | Val. rmse: 69.957703 |\n",
            "| Epoch: 10 | Train rmse: 70.243958 | Val. rmse: 68.237930 |\n",
            "| Epoch: 11 | Train rmse: 68.853790 | Val. rmse: 66.586609 |\n",
            "| Epoch: 12 | Train rmse: 67.087128 | Val. rmse: 65.002502 |\n",
            "| Epoch: 13 | Train rmse: 65.525093 | Val. rmse: 63.485504 |\n",
            "| Epoch: 14 | Train rmse: 63.757793 | Val. rmse: 62.034382 |\n",
            "| Epoch: 15 | Train rmse: 62.500763 | Val. rmse: 60.647991 |\n",
            "| Epoch: 16 | Train rmse: 60.910954 | Val. rmse: 59.325176 |\n",
            "| Epoch: 17 | Train rmse: 59.597233 | Val. rmse: 58.063904 |\n",
            "| Epoch: 18 | Train rmse: 58.248703 | Val. rmse: 56.863533 |\n",
            "| Epoch: 19 | Train rmse: 56.935577 | Val. rmse: 55.723244 |\n",
            "| Epoch: 20 | Train rmse: 55.613605 | Val. rmse: 54.642338 |\n",
            "| Epoch: 21 | Train rmse: 54.562294 | Val. rmse: 53.619347 |\n",
            "| Epoch: 22 | Train rmse: 53.543503 | Val. rmse: 52.653580 |\n",
            "| Epoch: 23 | Train rmse: 52.680832 | Val. rmse: 51.743759 |\n",
            "| Epoch: 24 | Train rmse: 51.656979 | Val. rmse: 50.889015 |\n",
            "| Epoch: 25 | Train rmse: 50.804317 | Val. rmse: 50.087883 |\n",
            "| Epoch: 26 | Train rmse: 49.984024 | Val. rmse: 49.339550 |\n",
            "| Epoch: 27 | Train rmse: 49.062073 | Val. rmse: 48.643124 |\n",
            "| Epoch: 28 | Train rmse: 48.413517 | Val. rmse: 47.997356 |\n",
            "| Epoch: 29 | Train rmse: 47.960239 | Val. rmse: 47.400791 |\n",
            "| Epoch: 30 | Train rmse: 47.351700 | Val. rmse: 46.852013 |\n",
            "| Epoch: 31 | Train rmse: 46.823410 | Val. rmse: 46.349567 |\n",
            "| Epoch: 32 | Train rmse: 46.103882 | Val. rmse: 45.892139 |\n",
            "| Epoch: 33 | Train rmse: 45.785088 | Val. rmse: 45.477966 |\n",
            "| Epoch: 34 | Train rmse: 45.164486 | Val. rmse: 45.105537 |\n",
            "| Epoch: 35 | Train rmse: 44.952110 | Val. rmse: 44.772903 |\n",
            "| Epoch: 36 | Train rmse: 44.649815 | Val. rmse: 44.478348 |\n",
            "| Epoch: 37 | Train rmse: 44.306110 | Val. rmse: 44.219879 |\n",
            "| Epoch: 38 | Train rmse: 44.211349 | Val. rmse: 43.995438 |\n",
            "| Epoch: 39 | Train rmse: 43.835506 | Val. rmse: 43.803009 |\n",
            "| Epoch: 40 | Train rmse: 43.667927 | Val. rmse: 43.640331 |\n",
            "| Epoch: 41 | Train rmse: 43.366146 | Val. rmse: 43.505119 |\n",
            "| Epoch: 42 | Train rmse: 43.325108 | Val. rmse: 43.395027 |\n",
            "| Epoch: 43 | Train rmse: 43.291214 | Val. rmse: 43.307625 |\n",
            "| Epoch: 44 | Train rmse: 43.165710 | Val. rmse: 43.240414 |\n",
            "| Epoch: 45 | Train rmse: 42.834877 | Val. rmse: 43.190857 |\n",
            "| Epoch: 46 | Train rmse: 42.784035 | Val. rmse: 43.156338 |\n",
            "| Epoch: 47 | Train rmse: 43.041233 | Val. rmse: 43.134174 |\n",
            "| Epoch: 48 | Train rmse: 42.942017 | Val. rmse: 43.121677 |\n",
            "| Epoch: 49 | Train rmse: 42.783306 | Val. rmse: 43.116116 |\n",
            "| Epoch: 50 | Train rmse: 42.808186 | Val. rmse: 43.114723 |\n",
            "| Epoch: 51 | Train rmse: 42.922436 | Val. rmse: 43.114723 |\n",
            "| Epoch: 52 | Train rmse: 42.974976 | Val. rmse: 43.113331 |\n",
            "| Epoch: 53 | Train rmse: 42.875942 | Val. rmse: 43.107758 |\n",
            "| Epoch: 54 | Train rmse: 43.036652 | Val. rmse: 43.095234 |\n",
            "| Epoch: 55 | Train rmse: 42.835678 | Val. rmse: 43.073021 |\n",
            "| Epoch: 56 | Train rmse: 42.860233 | Val. rmse: 43.038403 |\n",
            "| Epoch: 57 | Train rmse: 42.810207 | Val. rmse: 42.988720 |\n",
            "| Epoch: 58 | Train rmse: 42.850502 | Val. rmse: 42.921383 |\n",
            "| Epoch: 59 | Train rmse: 42.657925 | Val. rmse: 42.833858 |\n",
            "| Epoch: 60 | Train rmse: 42.657677 | Val. rmse: 42.723705 |\n",
            "| Epoch: 61 | Train rmse: 42.536522 | Val. rmse: 42.588638 |\n",
            "| Epoch: 62 | Train rmse: 42.339306 | Val. rmse: 42.426472 |\n",
            "| Epoch: 63 | Train rmse: 42.390106 | Val. rmse: 42.235065 |\n",
            "| Epoch: 64 | Train rmse: 42.031830 | Val. rmse: 42.012608 |\n",
            "| Epoch: 65 | Train rmse: 41.715691 | Val. rmse: 41.757286 |\n",
            "| Epoch: 66 | Train rmse: 41.658203 | Val. rmse: 41.467453 |\n",
            "| Epoch: 67 | Train rmse: 41.202412 | Val. rmse: 41.141815 |\n",
            "| Epoch: 68 | Train rmse: 40.902695 | Val. rmse: 40.779243 |\n",
            "| Epoch: 69 | Train rmse: 40.602821 | Val. rmse: 40.378674 |\n",
            "| Epoch: 70 | Train rmse: 40.098038 | Val. rmse: 39.939201 |\n",
            "| Epoch: 71 | Train rmse: 39.757637 | Val. rmse: 39.460335 |\n",
            "| Epoch: 72 | Train rmse: 39.085762 | Val. rmse: 38.942043 |\n",
            "| Epoch: 73 | Train rmse: 38.678196 | Val. rmse: 38.384182 |\n",
            "| Epoch: 74 | Train rmse: 38.235001 | Val. rmse: 37.786865 |\n",
            "| Epoch: 75 | Train rmse: 37.427238 | Val. rmse: 37.150707 |\n",
            "| Epoch: 76 | Train rmse: 36.802208 | Val. rmse: 36.476368 |\n",
            "| Epoch: 77 | Train rmse: 36.199833 | Val. rmse: 35.764637 |\n",
            "| Epoch: 78 | Train rmse: 35.476662 | Val. rmse: 35.016781 |\n",
            "| Epoch: 79 | Train rmse: 34.688160 | Val. rmse: 34.234295 |\n",
            "| Epoch: 80 | Train rmse: 33.915546 | Val. rmse: 33.418697 |\n",
            "| Epoch: 81 | Train rmse: 32.948967 | Val. rmse: 32.572468 |\n",
            "| Epoch: 82 | Train rmse: 32.168613 | Val. rmse: 31.697060 |\n",
            "| Epoch: 83 | Train rmse: 31.281605 | Val. rmse: 30.794556 |\n",
            "| Epoch: 84 | Train rmse: 30.445539 | Val. rmse: 29.867691 |\n",
            "| Epoch: 85 | Train rmse: 29.363970 | Val. rmse: 28.919285 |\n",
            "| Epoch: 86 | Train rmse: 28.544939 | Val. rmse: 27.951935 |\n",
            "| Epoch: 87 | Train rmse: 27.616913 | Val. rmse: 26.968201 |\n",
            "| Epoch: 88 | Train rmse: 26.663460 | Val. rmse: 25.970783 |\n",
            "| Epoch: 89 | Train rmse: 25.654116 | Val. rmse: 24.963030 |\n",
            "| Epoch: 90 | Train rmse: 24.523130 | Val. rmse: 23.948414 |\n",
            "| Epoch: 91 | Train rmse: 23.528435 | Val. rmse: 22.930252 |\n",
            "| Epoch: 92 | Train rmse: 22.616323 | Val. rmse: 21.911325 |\n",
            "| Epoch: 93 | Train rmse: 21.575066 | Val. rmse: 20.895193 |\n",
            "| Epoch: 94 | Train rmse: 20.496891 | Val. rmse: 19.884956 |\n",
            "| Epoch: 95 | Train rmse: 19.469097 | Val. rmse: 18.884222 |\n",
            "| Epoch: 96 | Train rmse: 18.594072 | Val. rmse: 17.895550 |\n",
            "| Epoch: 97 | Train rmse: 17.472065 | Val. rmse: 16.922096 |\n",
            "| Epoch: 98 | Train rmse: 16.477385 | Val. rmse: 15.967514 |\n",
            "| Epoch: 99 | Train rmse: 15.626856 | Val. rmse: 15.033501 |\n",
            "| Epoch: 100 | Train rmse: 14.707069 | Val. rmse: 14.123134 |\n",
            "| Epoch: 101 | Train rmse: 13.726530 | Val. rmse: 13.238502 |\n",
            "| Epoch: 102 | Train rmse: 12.882606 | Val. rmse: 12.382030 |\n",
            "| Epoch: 103 | Train rmse: 12.055025 | Val. rmse: 11.556147 |\n",
            "| Epoch: 104 | Train rmse: 11.192625 | Val. rmse: 10.762676 |\n",
            "| Epoch: 105 | Train rmse: 10.397299 | Val. rmse: 10.003138 |\n",
            "| Epoch: 106 | Train rmse: 9.666065 | Val. rmse: 9.278880 |\n",
            "| Epoch: 107 | Train rmse: 8.897113 | Val. rmse: 8.591165 |\n",
            "| Epoch: 108 | Train rmse: 8.260904 | Val. rmse: 7.940423 |\n",
            "| Epoch: 109 | Train rmse: 7.582620 | Val. rmse: 7.327024 |\n",
            "| Epoch: 110 | Train rmse: 6.956660 | Val. rmse: 6.751124 |\n",
            "| Epoch: 111 | Train rmse: 6.439545 | Val. rmse: 6.212086 |\n",
            "| Epoch: 112 | Train rmse: 5.831679 | Val. rmse: 5.710033 |\n",
            "| Epoch: 113 | Train rmse: 5.398197 | Val. rmse: 5.244308 |\n",
            "| Epoch: 114 | Train rmse: 4.937521 | Val. rmse: 4.813758 |\n",
            "| Epoch: 115 | Train rmse: 4.530829 | Val. rmse: 4.417052 |\n",
            "| Epoch: 116 | Train rmse: 4.155591 | Val. rmse: 4.052750 |\n",
            "| Epoch: 117 | Train rmse: 3.792531 | Val. rmse: 3.719635 |\n",
            "| Epoch: 118 | Train rmse: 3.432971 | Val. rmse: 3.416089 |\n",
            "| Epoch: 119 | Train rmse: 3.164514 | Val. rmse: 3.140244 |\n",
            "| Epoch: 120 | Train rmse: 2.905725 | Val. rmse: 2.890411 |\n",
            "| Epoch: 121 | Train rmse: 2.697782 | Val. rmse: 2.665145 |\n",
            "| Epoch: 122 | Train rmse: 2.457692 | Val. rmse: 2.462691 |\n",
            "| Epoch: 123 | Train rmse: 2.236152 | Val. rmse: 2.281839 |\n",
            "| Epoch: 124 | Train rmse: 2.114280 | Val. rmse: 2.120621 |\n",
            "| Epoch: 125 | Train rmse: 1.926348 | Val. rmse: 1.977760 |\n",
            "| Epoch: 126 | Train rmse: 1.806489 | Val. rmse: 1.851650 |\n",
            "| Epoch: 127 | Train rmse: 1.675032 | Val. rmse: 1.740984 |\n",
            "| Epoch: 128 | Train rmse: 1.575275 | Val. rmse: 1.644218 |\n",
            "| Epoch: 129 | Train rmse: 1.511266 | Val. rmse: 1.559750 |\n",
            "| Epoch: 130 | Train rmse: 1.431386 | Val. rmse: 1.486375 |\n",
            "| Epoch: 131 | Train rmse: 1.346947 | Val. rmse: 1.422960 |\n",
            "| Epoch: 132 | Train rmse: 1.290051 | Val. rmse: 1.368504 |\n",
            "| Epoch: 133 | Train rmse: 1.236983 | Val. rmse: 1.321920 |\n",
            "| Epoch: 134 | Train rmse: 1.206988 | Val. rmse: 1.282318 |\n",
            "| Epoch: 135 | Train rmse: 1.166878 | Val. rmse: 1.248707 |\n",
            "| Epoch: 136 | Train rmse: 1.132035 | Val. rmse: 1.220389 |\n",
            "| Epoch: 137 | Train rmse: 1.109838 | Val. rmse: 1.196676 |\n",
            "| Epoch: 138 | Train rmse: 1.086736 | Val. rmse: 1.176956 |\n",
            "| Epoch: 139 | Train rmse: 1.067458 | Val. rmse: 1.160685 |\n",
            "| Epoch: 140 | Train rmse: 1.057786 | Val. rmse: 1.147426 |\n",
            "| Epoch: 141 | Train rmse: 1.041007 | Val. rmse: 1.136769 |\n",
            "| Epoch: 142 | Train rmse: 1.029404 | Val. rmse: 1.128356 |\n",
            "| Epoch: 143 | Train rmse: 1.019815 | Val. rmse: 1.121857 |\n",
            "| Epoch: 144 | Train rmse: 1.016894 | Val. rmse: 1.116984 |\n",
            "| Epoch: 145 | Train rmse: 1.006604 | Val. rmse: 1.113475 |\n",
            "| Epoch: 146 | Train rmse: 1.009009 | Val. rmse: 1.111070 |\n",
            "| Epoch: 147 | Train rmse: 1.001667 | Val. rmse: 1.109555 |\n",
            "| Epoch: 148 | Train rmse: 1.000561 | Val. rmse: 1.108715 |\n",
            "| Epoch: 149 | Train rmse: 0.999800 | Val. rmse: 1.108346 |\n",
            "| Epoch: 150 | Train rmse: 1.000369 | Val. rmse: 1.108255 |\n",
            "| Epoch: 151 | Train rmse: 0.999269 | Val. rmse: 1.108255 |\n",
            "| Epoch: 152 | Train rmse: 1.001257 | Val. rmse: 1.108165 |\n",
            "| Epoch: 153 | Train rmse: 1.000002 | Val. rmse: 1.107810 |\n",
            "| Epoch: 154 | Train rmse: 0.995406 | Val. rmse: 1.107017 |\n",
            "| Epoch: 155 | Train rmse: 1.003358 | Val. rmse: 1.105621 |\n",
            "| Epoch: 156 | Train rmse: 1.003115 | Val. rmse: 1.103466 |\n",
            "| Epoch: 157 | Train rmse: 0.992512 | Val. rmse: 1.100402 |\n",
            "| Epoch: 158 | Train rmse: 1.001432 | Val. rmse: 1.096286 |\n",
            "| Epoch: 159 | Train rmse: 0.981122 | Val. rmse: 1.090995 |\n",
            "| Epoch: 160 | Train rmse: 0.979030 | Val. rmse: 1.084419 |\n",
            "| Epoch: 161 | Train rmse: 0.976825 | Val. rmse: 1.076494 |\n",
            "| Epoch: 162 | Train rmse: 0.985761 | Val. rmse: 1.067156 |\n",
            "| Epoch: 163 | Train rmse: 0.959397 | Val. rmse: 1.056373 |\n",
            "| Epoch: 164 | Train rmse: 0.957036 | Val. rmse: 1.044088 |\n",
            "| Epoch: 165 | Train rmse: 0.945776 | Val. rmse: 1.030356 |\n",
            "| Epoch: 166 | Train rmse: 0.946469 | Val. rmse: 1.015212 |\n",
            "| Epoch: 167 | Train rmse: 0.921394 | Val. rmse: 0.998775 |\n",
            "| Epoch: 168 | Train rmse: 0.900980 | Val. rmse: 0.981175 |\n",
            "| Epoch: 169 | Train rmse: 0.887420 | Val. rmse: 0.962574 |\n",
            "| Epoch: 170 | Train rmse: 0.871103 | Val. rmse: 0.943180 |\n",
            "| Epoch: 171 | Train rmse: 0.850115 | Val. rmse: 0.923216 |\n",
            "| Epoch: 172 | Train rmse: 0.841056 | Val. rmse: 0.902655 |\n",
            "| Epoch: 173 | Train rmse: 0.826145 | Val. rmse: 0.881783 |\n",
            "| Epoch: 174 | Train rmse: 0.809440 | Val. rmse: 0.860802 |\n",
            "| Epoch: 175 | Train rmse: 0.789745 | Val. rmse: 0.839911 |\n",
            "| Epoch: 176 | Train rmse: 0.765168 | Val. rmse: 0.819417 |\n",
            "| Epoch: 177 | Train rmse: 0.758834 | Val. rmse: 0.799362 |\n",
            "| Epoch: 178 | Train rmse: 0.739919 | Val. rmse: 0.779914 |\n",
            "| Epoch: 179 | Train rmse: 0.716656 | Val. rmse: 0.761449 |\n",
            "| Epoch: 180 | Train rmse: 0.714494 | Val. rmse: 0.744084 |\n",
            "| Epoch: 181 | Train rmse: 0.704495 | Val. rmse: 0.727985 |\n",
            "| Epoch: 182 | Train rmse: 0.687254 | Val. rmse: 0.713104 |\n",
            "| Epoch: 183 | Train rmse: 0.676321 | Val. rmse: 0.699467 |\n",
            "| Epoch: 184 | Train rmse: 0.669951 | Val. rmse: 0.687048 |\n",
            "| Epoch: 185 | Train rmse: 0.657049 | Val. rmse: 0.676263 |\n",
            "| Epoch: 186 | Train rmse: 0.651675 | Val. rmse: 0.666809 |\n",
            "| Epoch: 187 | Train rmse: 0.641072 | Val. rmse: 0.658537 |\n",
            "| Epoch: 188 | Train rmse: 0.642187 | Val. rmse: 0.651282 |\n",
            "| Epoch: 189 | Train rmse: 0.632192 | Val. rmse: 0.644892 |\n",
            "| Epoch: 190 | Train rmse: 0.630428 | Val. rmse: 0.639288 |\n",
            "| Epoch: 191 | Train rmse: 0.624200 | Val. rmse: 0.634542 |\n",
            "| Epoch: 192 | Train rmse: 0.624410 | Val. rmse: 0.630660 |\n",
            "| Epoch: 193 | Train rmse: 0.621227 | Val. rmse: 0.627742 |\n",
            "| Epoch: 194 | Train rmse: 0.614335 | Val. rmse: 0.625518 |\n",
            "| Epoch: 195 | Train rmse: 0.609242 | Val. rmse: 0.623786 |\n",
            "| Epoch: 196 | Train rmse: 0.608025 | Val. rmse: 0.622778 |\n",
            "| Epoch: 197 | Train rmse: 0.604286 | Val. rmse: 0.621762 |\n",
            "| Epoch: 198 | Train rmse: 0.601526 | Val. rmse: 0.621047 |\n",
            "| Epoch: 199 | Train rmse: 0.602797 | Val. rmse: 0.620780 |\n",
            "| Epoch: 200 | Train rmse: 0.600670 | Val. rmse: 0.620577 |\n",
            "| Epoch: 201 | Train rmse: 0.593657 | Val. rmse: 0.620271 |\n",
            "| Epoch: 202 | Train rmse: 0.588862 | Val. rmse: 0.619658 |\n",
            "| Epoch: 203 | Train rmse: 0.592941 | Val. rmse: 0.619060 |\n",
            "| Epoch: 204 | Train rmse: 0.585364 | Val. rmse: 0.618928 |\n",
            "| Epoch: 205 | Train rmse: 0.576739 | Val. rmse: 0.619183 |\n",
            "| Epoch: 206 | Train rmse: 0.586373 | Val. rmse: 0.619498 |\n",
            "| Epoch: 207 | Train rmse: 0.581223 | Val. rmse: 0.619798 |\n",
            "| Epoch: 208 | Train rmse: 0.582569 | Val. rmse: 0.620074 |\n",
            "| Epoch: 209 | Train rmse: 0.576033 | Val. rmse: 0.619684 |\n",
            "| Epoch: 210 | Train rmse: 0.579062 | Val. rmse: 0.619390 |\n",
            "| Epoch: 211 | Train rmse: 0.573736 | Val. rmse: 0.619094 |\n",
            "| Epoch: 212 | Train rmse: 0.572471 | Val. rmse: 0.619053 |\n",
            "| Epoch: 213 | Train rmse: 0.574272 | Val. rmse: 0.619062 |\n",
            "| Epoch: 214 | Train rmse: 0.569012 | Val. rmse: 0.619072 |\n",
            "| Epoch: 215 | Train rmse: 0.570187 | Val. rmse: 0.619166 |\n",
            "| Epoch: 216 | Train rmse: 0.566289 | Val. rmse: 0.619165 |\n",
            "| Epoch: 217 | Train rmse: 0.561319 | Val. rmse: 0.618942 |\n",
            "| Epoch: 218 | Train rmse: 0.563847 | Val. rmse: 0.618731 |\n",
            "| Epoch: 219 | Train rmse: 0.563837 | Val. rmse: 0.618384 |\n",
            "| Epoch: 220 | Train rmse: 0.556939 | Val. rmse: 0.617918 |\n",
            "| Epoch: 221 | Train rmse: 0.565913 | Val. rmse: 0.617792 |\n",
            "| Epoch: 222 | Train rmse: 0.561561 | Val. rmse: 0.617769 |\n",
            "| Epoch: 223 | Train rmse: 0.560782 | Val. rmse: 0.617971 |\n",
            "| Epoch: 224 | Train rmse: 0.564120 | Val. rmse: 0.618207 |\n",
            "| Epoch: 225 | Train rmse: 0.562153 | Val. rmse: 0.618391 |\n",
            "| Epoch: 226 | Train rmse: 0.559240 | Val. rmse: 0.618307 |\n",
            "| Epoch: 227 | Train rmse: 0.559201 | Val. rmse: 0.618116 |\n",
            "| Epoch: 228 | Train rmse: 0.556641 | Val. rmse: 0.617946 |\n",
            "| Epoch: 229 | Train rmse: 0.559683 | Val. rmse: 0.617941 |\n",
            "| Epoch: 230 | Train rmse: 0.555321 | Val. rmse: 0.617950 |\n",
            "| Epoch: 231 | Train rmse: 0.556296 | Val. rmse: 0.617996 |\n",
            "| Epoch: 232 | Train rmse: 0.556605 | Val. rmse: 0.618012 |\n",
            "| Epoch: 233 | Train rmse: 0.554511 | Val. rmse: 0.617968 |\n",
            "| Epoch: 234 | Train rmse: 0.554526 | Val. rmse: 0.617916 |\n",
            "| Epoch: 235 | Train rmse: 0.557527 | Val. rmse: 0.617888 |\n",
            "| Epoch: 236 | Train rmse: 0.551324 | Val. rmse: 0.617866 |\n",
            "| Epoch: 237 | Train rmse: 0.551146 | Val. rmse: 0.617823 |\n",
            "| Epoch: 238 | Train rmse: 0.549122 | Val. rmse: 0.617787 |\n",
            "| Epoch: 239 | Train rmse: 0.550496 | Val. rmse: 0.617702 |\n",
            "| Epoch: 240 | Train rmse: 0.550194 | Val. rmse: 0.617572 |\n",
            "| Epoch: 241 | Train rmse: 0.553806 | Val. rmse: 0.617479 |\n",
            "| Epoch: 242 | Train rmse: 0.556974 | Val. rmse: 0.617407 |\n",
            "| Epoch: 243 | Train rmse: 0.551279 | Val. rmse: 0.617346 |\n",
            "| Epoch: 244 | Train rmse: 0.546245 | Val. rmse: 0.617301 |\n",
            "| Epoch: 245 | Train rmse: 0.547854 | Val. rmse: 0.617261 |\n",
            "| Epoch: 246 | Train rmse: 0.556222 | Val. rmse: 0.617230 |\n",
            "| Epoch: 247 | Train rmse: 0.549609 | Val. rmse: 0.617210 |\n",
            "| Epoch: 248 | Train rmse: 0.552057 | Val. rmse: 0.617196 |\n",
            "| Epoch: 249 | Train rmse: 0.548689 | Val. rmse: 0.617189 |\n",
            "| Epoch: 250 | Train rmse: 0.550814 | Val. rmse: 0.617187 |\n",
            "| Epoch: 251 | Train rmse: 0.551470 | Val. rmse: 0.617187 |\n",
            "| Epoch: 252 | Train rmse: 0.548772 | Val. rmse: 0.617186 |\n",
            "| Epoch: 253 | Train rmse: 0.549132 | Val. rmse: 0.617182 |\n",
            "| Epoch: 254 | Train rmse: 0.543886 | Val. rmse: 0.617173 |\n",
            "| Epoch: 255 | Train rmse: 0.549796 | Val. rmse: 0.617157 |\n",
            "| Epoch: 256 | Train rmse: 0.549145 | Val. rmse: 0.617128 |\n",
            "| Epoch: 257 | Train rmse: 0.552058 | Val. rmse: 0.617089 |\n",
            "| Epoch: 258 | Train rmse: 0.551672 | Val. rmse: 0.617042 |\n",
            "| Epoch: 259 | Train rmse: 0.551709 | Val. rmse: 0.616981 |\n",
            "| Epoch: 260 | Train rmse: 0.553789 | Val. rmse: 0.616915 |\n",
            "| Epoch: 261 | Train rmse: 0.553905 | Val. rmse: 0.616844 |\n",
            "| Epoch: 262 | Train rmse: 0.551464 | Val. rmse: 0.616766 |\n",
            "| Epoch: 263 | Train rmse: 0.547815 | Val. rmse: 0.616666 |\n",
            "| Epoch: 264 | Train rmse: 0.550761 | Val. rmse: 0.616579 |\n",
            "| Epoch: 265 | Train rmse: 0.550376 | Val. rmse: 0.616487 |\n",
            "| Epoch: 266 | Train rmse: 0.547213 | Val. rmse: 0.616442 |\n",
            "| Epoch: 267 | Train rmse: 0.554268 | Val. rmse: 0.616405 |\n",
            "| Epoch: 268 | Train rmse: 0.547233 | Val. rmse: 0.616410 |\n",
            "| Epoch: 269 | Train rmse: 0.550090 | Val. rmse: 0.616450 |\n",
            "| Epoch: 270 | Train rmse: 0.550413 | Val. rmse: 0.616461 |\n",
            "| Epoch: 271 | Train rmse: 0.548535 | Val. rmse: 0.616447 |\n",
            "| Epoch: 272 | Train rmse: 0.553554 | Val. rmse: 0.616593 |\n",
            "| Epoch: 273 | Train rmse: 0.548992 | Val. rmse: 0.616697 |\n",
            "| Epoch: 274 | Train rmse: 0.549255 | Val. rmse: 0.616864 |\n",
            "| Epoch: 275 | Train rmse: 0.547964 | Val. rmse: 0.617105 |\n",
            "| Epoch: 276 | Train rmse: 0.544328 | Val. rmse: 0.617447 |\n",
            "| Epoch: 277 | Train rmse: 0.547996 | Val. rmse: 0.617857 |\n",
            "| Epoch: 278 | Train rmse: 0.542389 | Val. rmse: 0.618243 |\n",
            "| Epoch: 279 | Train rmse: 0.549108 | Val. rmse: 0.618600 |\n",
            "| Epoch: 280 | Train rmse: 0.543756 | Val. rmse: 0.618935 |\n",
            "| Epoch: 281 | Train rmse: 0.543314 | Val. rmse: 0.619264 |\n",
            "| Epoch: 282 | Train rmse: 0.543087 | Val. rmse: 0.619587 |\n",
            "| Epoch: 283 | Train rmse: 0.540834 | Val. rmse: 0.619795 |\n",
            "| Epoch: 284 | Train rmse: 0.545501 | Val. rmse: 0.619970 |\n",
            "| Epoch: 285 | Train rmse: 0.543483 | Val. rmse: 0.620098 |\n",
            "| Epoch: 286 | Train rmse: 0.541614 | Val. rmse: 0.620453 |\n",
            "| Epoch: 287 | Train rmse: 0.542104 | Val. rmse: 0.620910 |\n",
            "| Epoch: 288 | Train rmse: 0.537617 | Val. rmse: 0.620853 |\n",
            "| Epoch: 289 | Train rmse: 0.543295 | Val. rmse: 0.620820 |\n",
            "| Epoch: 290 | Train rmse: 0.540547 | Val. rmse: 0.620873 |\n",
            "| Epoch: 291 | Train rmse: 0.542709 | Val. rmse: 0.621241 |\n",
            "| Epoch: 292 | Train rmse: 0.538375 | Val. rmse: 0.621427 |\n",
            "| Epoch: 293 | Train rmse: 0.540816 | Val. rmse: 0.621483 |\n",
            "| Epoch: 294 | Train rmse: 0.542958 | Val. rmse: 0.621449 |\n",
            "| Epoch: 295 | Train rmse: 0.537628 | Val. rmse: 0.621510 |\n",
            "| Epoch: 296 | Train rmse: 0.542227 | Val. rmse: 0.621598 |\n",
            "| Epoch: 297 | Train rmse: 0.540607 | Val. rmse: 0.621529 |\n",
            "| Epoch: 298 | Train rmse: 0.536571 | Val. rmse: 0.621355 |\n",
            "| Epoch: 299 | Train rmse: 0.535151 | Val. rmse: 0.621359 |\n",
            "| Epoch: 300 | Train rmse: 0.535309 | Val. rmse: 0.621330 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFTSf8gNUKB3",
        "outputId": "51e4b4b3-ed70-4773-a83f-d74f108d2ff2"
      },
      "source": [
        "#####Check the test RMSE####\n",
        "modelCNN1.eval()\n",
        "\n",
        "test_x_feature = orgTensorT.to(device)\n",
        "test_target = labelTensorT.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_predictions = modelCNN1(test_x_feature).squeeze(1)\n",
        "    test_loss = torch.sqrt(((test_predictions - test_target)**2).mean()).item()\n",
        "\n",
        "print(f'train rmse: {train_loss:.6f} ; test. rmse: {test_loss:.6f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train rmse: 0.535309 ; test. rmse: 0.632448\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37oHhcLUkovE"
      },
      "source": [
        "### BiLSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwu5TJU2lCxe",
        "outputId": "d3506fe6-2676-4f61-80ca-55739259e909"
      },
      "source": [
        "# Set up hyperparameters\n",
        "INPUT_DIM = len(word2idx)\n",
        "EMBEDDING_DIM = 30\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "modelBiLSTM = BiLSTM(EMBEDDING_DIM, 10, INPUT_DIM, BATCH_SIZE, device)\n",
        "print(\"BiLSTM Model initialised.\")\n",
        "\n",
        "modelBiLSTM.to(device)\n",
        "\n",
        "print(\"Dataloaders created.\")\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "loss_fn = loss_fn.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(modelBiLSTM.parameters(),0.0001)\n",
        "\n",
        "# model training\n",
        "# original sentence as input\n",
        "train1in(train_dataloader_1Org, valid_dataloader_1Org, modelBiLSTM, 12)\n",
        "# Edited sentence as input\n",
        "#train1in(train_dataloader_1Edt, valid_dataloader_1Edt, modelBiLSTM, 8)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BiLSTM Model initialised.\n",
            "Dataloaders created.\n",
            "Training model.\n",
            "| Epoch: 01 | Train Loss: 1.26 | Train MSE: 1.26 | Train RMSE: 1.12370 |         Val. Loss: 1.14 | Val. MSE: 1.14 |  Val. RMSE: 1.06960 |\n",
            "| Epoch: 02 | Train Loss: 0.96 | Train MSE: 0.96 | Train RMSE: 0.97847 |         Val. Loss: 0.79 | Val. MSE: 0.79 |  Val. RMSE: 0.89110 |\n",
            "| Epoch: 03 | Train Loss: 0.57 | Train MSE: 0.57 | Train RMSE: 0.75269 |         Val. Loss: 0.38 | Val. MSE: 0.38 |  Val. RMSE: 0.61858 |\n",
            "| Epoch: 04 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.56509 |         Val. Loss: 0.32 | Val. MSE: 0.32 |  Val. RMSE: 0.56151 |\n",
            "| Epoch: 05 | Train Loss: 0.31 | Train MSE: 0.31 | Train RMSE: 0.55326 |         Val. Loss: 0.31 | Val. MSE: 0.31 |  Val. RMSE: 0.56107 |\n",
            "| Epoch: 06 | Train Loss: 0.31 | Train MSE: 0.31 | Train RMSE: 0.55294 |         Val. Loss: 0.31 | Val. MSE: 0.31 |  Val. RMSE: 0.56087 |\n",
            "| Epoch: 07 | Train Loss: 0.31 | Train MSE: 0.31 | Train RMSE: 0.55266 |         Val. Loss: 0.31 | Val. MSE: 0.31 |  Val. RMSE: 0.56081 |\n",
            "| Epoch: 08 | Train Loss: 0.31 | Train MSE: 0.31 | Train RMSE: 0.55240 |         Val. Loss: 0.31 | Val. MSE: 0.31 |  Val. RMSE: 0.56088 |\n",
            "| Epoch: 09 | Train Loss: 0.30 | Train MSE: 0.30 | Train RMSE: 0.55216 |         Val. Loss: 0.31 | Val. MSE: 0.31 |  Val. RMSE: 0.56078 |\n",
            "| Epoch: 10 | Train Loss: 0.30 | Train MSE: 0.30 | Train RMSE: 0.55190 |         Val. Loss: 0.31 | Val. MSE: 0.31 |  Val. RMSE: 0.56059 |\n",
            "| Epoch: 11 | Train Loss: 0.30 | Train MSE: 0.30 | Train RMSE: 0.55171 |         Val. Loss: 0.31 | Val. MSE: 0.31 |  Val. RMSE: 0.56047 |\n",
            "| Epoch: 12 | Train Loss: 0.30 | Train MSE: 0.30 | Train RMSE: 0.55147 |         Val. Loss: 0.31 | Val. MSE: 0.31 |  Val. RMSE: 0.56040 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8PqYemBU3Uq",
        "outputId": "aaa623c6-ac08-494e-a752-1bd4178be58c"
      },
      "source": [
        "#####Check the test RMSE####\n",
        "\n",
        "BiLSTM_Org_predictions = torch.empty(0)\n",
        "BiLSTM_Org_predictions = BiLSTM_Org_predictions.to(device)\n",
        "test_target = labelTensorT.to(device)\n",
        "\n",
        "modelBiLSTM.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "  for batch in test_dataloader_org:\n",
        "    feature = batch\n",
        "    feature = feature.to(device)\n",
        "    modelBiLSTM.batch_size = feature.shape[0]\n",
        "    modelBiLSTM.hidden = modelBiLSTM.init_hidden()\n",
        "    pdc = modelBiLSTM(feature).squeeze(1)\n",
        "    BiLSTM_Org_predictions = torch.cat((BiLSTM_Org_predictions,pdc), 0)\n",
        "\n",
        "test_loss = torch.sqrt(((BiLSTM_Org_predictions - test_target)**2).mean()).item()\n",
        "print(f'test. rmse: {test_loss:.6f}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test. rmse: 0.578346\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_hY_4GdwIQX"
      },
      "source": [
        "### 2 inputs GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMNE0cfzwXQO",
        "outputId": "e26b3abf-dd9b-49b2-a197-baa2681468fc"
      },
      "source": [
        "# Set up hyperparameters\n",
        "INPUT_DIM = len(word2idx)\n",
        "EMBEDDING_DIM = 300\n",
        "BATCH_SIZE = 12\n",
        "\n",
        "modelgru = twoInputGRU(25, 10, INPUT_DIM, 8, device)\n",
        "print(\"GRU Model initialised.\")\n",
        "\n",
        "modelgru.to(device)\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "loss_fn = loss_fn.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(modelgru.parameters(),lr=0.001)\n",
        "\n",
        "# steps = 8\n",
        "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, steps)\n",
        "\n",
        "train2in(train_dataloader, valid_dataloader, modelgru, 10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GRU Model initialised.\n",
            "Training model.\n",
            "| Epoch: 01 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.56182 |         Val. Loss: 0.32 | Val. MSE: 0.32 |  Val. RMSE: 0.56549 |\n",
            "| Epoch: 02 | Train Loss: 0.30 | Train MSE: 0.30 | Train RMSE: 0.54849 |         Val. Loss: 0.33 | Val. MSE: 0.33 |  Val. RMSE: 0.57029 |\n",
            "| Epoch: 03 | Train Loss: 0.29 | Train MSE: 0.29 | Train RMSE: 0.54060 |         Val. Loss: 0.33 | Val. MSE: 0.33 |  Val. RMSE: 0.57859 |\n",
            "| Epoch: 04 | Train Loss: 0.28 | Train MSE: 0.28 | Train RMSE: 0.52970 |         Val. Loss: 0.39 | Val. MSE: 0.39 |  Val. RMSE: 0.62508 |\n",
            "| Epoch: 05 | Train Loss: 0.28 | Train MSE: 0.28 | Train RMSE: 0.52818 |         Val. Loss: 0.34 | Val. MSE: 0.34 |  Val. RMSE: 0.58455 |\n",
            "| Epoch: 06 | Train Loss: 0.26 | Train MSE: 0.26 | Train RMSE: 0.51133 |         Val. Loss: 0.35 | Val. MSE: 0.35 |  Val. RMSE: 0.59098 |\n",
            "| Epoch: 07 | Train Loss: 0.28 | Train MSE: 0.28 | Train RMSE: 0.52492 |         Val. Loss: 0.35 | Val. MSE: 0.35 |  Val. RMSE: 0.59458 |\n",
            "| Epoch: 08 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58010 |         Val. Loss: 0.37 | Val. MSE: 0.37 |  Val. RMSE: 0.61047 |\n",
            "| Epoch: 09 | Train Loss: 0.30 | Train MSE: 0.30 | Train RMSE: 0.54423 |         Val. Loss: 0.35 | Val. MSE: 0.35 |  Val. RMSE: 0.58980 |\n",
            "| Epoch: 10 | Train Loss: 0.28 | Train MSE: 0.28 | Train RMSE: 0.52939 |         Val. Loss: 0.36 | Val. MSE: 0.36 |  Val. RMSE: 0.59638 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGCuzA_rWMhD",
        "outputId": "9c9bb781-0cf3-4c9a-a7df-4d823762e700"
      },
      "source": [
        "#GRU model with 2 inputs\n",
        "GRU_2in_predictions = torch.empty(0)\n",
        "GRU_2in_predictions = GRU_2in_predictions.to(device)\n",
        "test_target = labelTensorT.to(device)\n",
        "\n",
        "modelgru.eval()\n",
        "with torch.no_grad():\n",
        "  for batch in test_dataloader_2in:\n",
        "    feature1,feature2 = batch\n",
        "    feature1,feature2 = feature1.to(device),feature1.to(device)\n",
        "    modelgru.batch_size = feature1.shape[0]\n",
        "\n",
        "    modelgru.hidden = modelgru.init_hidden()\n",
        "\n",
        "    pdc = modelgru(feature1,feature2).squeeze(1)\n",
        "    GRU_2in_predictions = torch.cat((GRU_2in_predictions,pdc), 0)\n",
        "\n",
        "test_loss = torch.sqrt(((GRU_2in_predictions - test_target)**2).mean()).item()\n",
        "print(f'test. rmse: {test_loss:.6f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test. rmse: 0.617449\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CS8Q6mAHoBjF"
      },
      "source": [
        "### 1 input GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8uTAbXGwJ4Y",
        "outputId": "e2d9c799-410d-4337-8731-d33d83704b3f"
      },
      "source": [
        "modelgru2 = GRU(25, 10, INPUT_DIM, BATCH_SIZE, device)\n",
        "\n",
        "modelgru2.to(device)\n",
        "\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "loss_fn = loss_fn.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(modelgru2.parameters(),0.0001)\n",
        "\n",
        "#original sentence as input\n",
        "train1in(train_dataloader_1Org, valid_dataloader_1Org, modelgru2, 12)\n",
        "\n",
        "#edited sentence as input\n",
        "#train1in(train_dataloader_1Edt, valid_dataloader_1Edt, modelgru2, 12)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training model.\n",
            "| Epoch: 01 | Train Loss: 0.96 | Train MSE: 0.96 | Train RMSE: 0.98009 |         Val. Loss: 0.82 | Val. MSE: 0.82 |  Val. RMSE: 0.90603 |\n",
            "| Epoch: 02 | Train Loss: 0.66 | Train MSE: 0.66 | Train RMSE: 0.81131 |         Val. Loss: 0.54 | Val. MSE: 0.54 |  Val. RMSE: 0.73646 |\n",
            "| Epoch: 03 | Train Loss: 0.43 | Train MSE: 0.43 | Train RMSE: 0.65531 |         Val. Loss: 0.37 | Val. MSE: 0.37 |  Val. RMSE: 0.60543 |\n",
            "| Epoch: 04 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.56934 |         Val. Loss: 0.32 | Val. MSE: 0.32 |  Val. RMSE: 0.56406 |\n",
            "| Epoch: 05 | Train Loss: 0.31 | Train MSE: 0.31 | Train RMSE: 0.55356 |         Val. Loss: 0.31 | Val. MSE: 0.31 |  Val. RMSE: 0.56071 |\n",
            "| Epoch: 06 | Train Loss: 0.31 | Train MSE: 0.31 | Train RMSE: 0.55272 |         Val. Loss: 0.31 | Val. MSE: 0.31 |  Val. RMSE: 0.56057 |\n",
            "| Epoch: 07 | Train Loss: 0.31 | Train MSE: 0.31 | Train RMSE: 0.55254 |         Val. Loss: 0.31 | Val. MSE: 0.31 |  Val. RMSE: 0.56054 |\n",
            "| Epoch: 08 | Train Loss: 0.31 | Train MSE: 0.31 | Train RMSE: 0.55236 |         Val. Loss: 0.31 | Val. MSE: 0.31 |  Val. RMSE: 0.56052 |\n",
            "| Epoch: 09 | Train Loss: 0.30 | Train MSE: 0.30 | Train RMSE: 0.55219 |         Val. Loss: 0.31 | Val. MSE: 0.31 |  Val. RMSE: 0.56041 |\n",
            "| Epoch: 10 | Train Loss: 0.30 | Train MSE: 0.30 | Train RMSE: 0.55205 |         Val. Loss: 0.31 | Val. MSE: 0.31 |  Val. RMSE: 0.56052 |\n",
            "| Epoch: 11 | Train Loss: 0.30 | Train MSE: 0.30 | Train RMSE: 0.55186 |         Val. Loss: 0.31 | Val. MSE: 0.31 |  Val. RMSE: 0.56050 |\n",
            "| Epoch: 12 | Train Loss: 0.30 | Train MSE: 0.30 | Train RMSE: 0.55164 |         Val. Loss: 0.31 | Val. MSE: 0.31 |  Val. RMSE: 0.56047 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvtHKCF9WeB9",
        "outputId": "40dc8a2f-252e-4139-8fee-58c20c647dea"
      },
      "source": [
        "#GRU model with 1 inputs\n",
        "GRU_1in_predictions = torch.empty(0)\n",
        "GRU_1in_predictions = GRU_1in_predictions.to(device)\n",
        "test_target = labelTensorT.to(device)\n",
        "\n",
        "modelgru2.eval()\n",
        "with torch.no_grad():\n",
        "  #orginial sentence as input\n",
        "  for batch in test_dataloader_org:\n",
        "  #edited sentence as input\n",
        "  #for batch in test_dataseter_edt:\n",
        "    feature = batch\n",
        "    feature = feature.to(device)\n",
        "    modelgru2.batch_size = feature.shape[0]\n",
        "    modelgru2.hidden = modelgru2.init_hidden()\n",
        "    pdc = modelgru2(feature).squeeze(1)\n",
        "    GRU_1in_predictions = torch.cat((GRU_1in_predictions,pdc), 0)\n",
        "\n",
        "test_loss = torch.sqrt(((GRU_1in_predictions - test_target)**2).mean()).item()   \n",
        "print(f'test. rmse: {test_loss:.6f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test. rmse: 0.577395\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWZ8CEt_UmPF"
      },
      "source": [
        "# In order to facilitate the viewing of the results, we summarize the tests here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0H20ucdh4J1j"
      },
      "source": [
        "### CNN\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3YtJqOc5BT-",
        "outputId": "1e4d15ac-1432-4511-e087-edfd8366b52d"
      },
      "source": [
        "orgSenTest = orgTensorT.to(device)\n",
        "edtSenTest = editTensorT.to(device)\n",
        "test_target = labelTensorT.to(device)\n",
        "\n",
        "# 2 inputs CNN model\n",
        "modelCNN.eval()\n",
        "with torch.no_grad():\n",
        "  cnn_test_predictions = modelCNN(orgSenTest, edtSenTest).squeeze(1)\n",
        "  test_loss = torch.sqrt(((cnn_test_predictions - test_target)**2).mean()).item()\n",
        "print(f'2 input CNN: test. rmse: {test_loss:.6f} ')\n",
        "\n",
        "# 1 input CNN model\n",
        "modelCNN1.eval()\n",
        "with torch.no_grad():\n",
        "  # original sentence input cnn model\n",
        "  cnn_ori_test_predictions = modelCNN1(orgSenTest).squeeze(1)\n",
        "  test_loss = torch.sqrt(((cnn_ori_test_predictions - test_target)**2).mean()).item()\n",
        "  # edit sentence input cnn model\n",
        "  #cnn_edit_test_predictions = modelCNN1(edtSenTest).squeeze(1)\n",
        "print(f'1 input CNN: test. rmse: {test_loss:.6f} ')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2 input CNN: test. rmse: 0.611087 \n",
            "1 input CNN: test. rmse: 0.632448 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0qU3qov_rnn"
      },
      "source": [
        "### BiLSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_bdLcXK_uK7",
        "outputId": "3d616121-755b-4c6b-db09-692d5689b80d"
      },
      "source": [
        "#biLSTM model with original sentence as only one input\n",
        "BiLSTM_Org_predictions = torch.empty(0)\n",
        "BiLSTM_Org_predictions = BiLSTM_Org_predictions.to(device)\n",
        "test_target = labelTensorT.to(device)\n",
        "\n",
        "modelBiLSTM.eval()\n",
        "with torch.no_grad():\n",
        "  for batch in test_dataloader_org:\n",
        "    feature = batch\n",
        "    feature = feature.to(device)\n",
        "    modelBiLSTM.batch_size = feature.shape[0]\n",
        "\n",
        "    modelBiLSTM.hidden = modelBiLSTM.init_hidden()\n",
        "\n",
        "    pdc = modelBiLSTM(feature).squeeze(1)\n",
        "    BiLSTM_Org_predictions = torch.cat((BiLSTM_Org_predictions,pdc), 0)\n",
        "test_loss = torch.sqrt(((BiLSTM_Org_predictions - test_target)**2).mean()).item()\n",
        "print(f'1 input BiLstm with original sentence: Test rmse: {test_loss:.6f} ')\n",
        "\n",
        "#biLSTM model with edited sentence as only one input\n",
        "BiLSTM_Edt_predictions = torch.empty(0)\n",
        "BiLSTM_Edt_predictions = BiLSTM_Edt_predictions.to(device)\n",
        "modelBiLSTM.eval()\n",
        "with torch.no_grad():\n",
        "  for batch in test_dataloader_edt:\n",
        "    feature = batch\n",
        "    feature = feature.to(device)\n",
        "    modelBiLSTM.batch_size = feature.shape[0]\n",
        "\n",
        "    modelBiLSTM.hidden = modelBiLSTM.init_hidden()\n",
        "\n",
        "    pdc = modelBiLSTM(feature).squeeze(1)\n",
        "    BiLSTM_Edt_predictions = torch.cat((BiLSTM_Edt_predictions,pdc), 0)\n",
        "test_loss = torch.sqrt(((BiLSTM_Edt_predictions - test_target)**2).mean()).item()\n",
        "print(f'1 input BiLstm with edited sentence: Test rmse: {test_loss:.6f} ')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 input BiLstm with original sentence: Test rmse: 0.578346 \n",
            "1 input BiLstm with edited sentence: Test rmse: 0.578442 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3iYFfBW0CQL"
      },
      "source": [
        "### GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLcGG6g60FJ5",
        "outputId": "1d2b6d06-af5c-4a24-c752-58d3b44d682a"
      },
      "source": [
        "#GRU model with 2 inputs\n",
        "GRU_2in_predictions = torch.empty(0)\n",
        "GRU_2in_predictions = GRU_2in_predictions.to(device)\n",
        "test_target = labelTensorT.to(device)\n",
        "\n",
        "modelgru.eval()\n",
        "with torch.no_grad():\n",
        "  for batch in test_dataloader_2in:\n",
        "    feature1,feature2 = batch\n",
        "    feature1,feature2 = feature1.to(device),feature1.to(device)\n",
        "    modelgru.batch_size = feature1.shape[0]\n",
        "\n",
        "    modelgru.hidden = modelgru.init_hidden()\n",
        "\n",
        "    pdc = modelgru(feature1,feature2).squeeze(1)\n",
        "    GRU_2in_predictions = torch.cat((GRU_2in_predictions,pdc), 0)\n",
        "\n",
        "test_loss = torch.sqrt(((GRU_2in_predictions - test_target)**2).mean()).item()\n",
        "print(f'2 input GRU: Test rmse: {test_loss:.6f} ')\n",
        "\n",
        "#GRU model with 1 inputs\n",
        "GRU_1in_predictions = torch.empty(0)\n",
        "GRU_1in_predictions = GRU_1in_predictions.to(device)\n",
        "modelgru2.eval()\n",
        "with torch.no_grad():\n",
        "  #orginial sentence as input\n",
        "  #(if edited sentence as input, change the code)\n",
        "  #(for batch in test_dataseter_edt:)\n",
        "  for batch in test_dataloader_org:\n",
        "    feature = batch\n",
        "    feature = feature.to(device)\n",
        "    modelgru2.batch_size = feature.shape[0]\n",
        "    modelgru2.hidden = modelgru2.init_hidden()\n",
        "    pdc = modelgru2(feature).squeeze(1)\n",
        "    GRU_1in_predictions = torch.cat((GRU_1in_predictions,pdc), 0)\n",
        "\n",
        "test_loss = torch.sqrt(((GRU_1in_predictions - test_target)**2).mean()).item()\n",
        "print(f'1 input GRU: Test rmse: {test_loss:.6f} ')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2 input GRU: Test rmse: 0.617449 \n",
            "1 input GRU: Test rmse: 0.577395 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9DPGvMgEB6_"
      },
      "source": [
        "## output predicted file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tf0oRUKRELkX"
      },
      "source": [
        "## write down! ##\n",
        "def write_down(predictions, test_data_frame, out_loc):\n",
        "    test_data_frame['pred'] = predictions\n",
        "    output = test_data_frame[['id','pred']]\n",
        "    output.to_csv(out_loc, index=False)\n",
        "    print('output finished, address: '+os.path.abspath(out_loc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tv-AaD33EUSH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5fe8791-5e32-4f09-dca0-e79f87cd82a6"
      },
      "source": [
        "baselinePre = torch.from_numpy(predictedT)\n",
        "baseline = 'baselineScore.csv'\n",
        "write_down(baselinePre, test_df, baseline)\n",
        "\n",
        "GRU_2in = 'gru2in.csv'\n",
        "write_down(GRU_2in_predictions.cpu(), test_df, GRU_2in)\n",
        "\n",
        "GRU_1in_org = 'gru1in.csv'\n",
        "write_down(GRU_1in_predictions.cpu(), test_df, GRU_1in_org)\n",
        "\n",
        "BiLSTM_1in_org = 'bilstm1in_org.csv'\n",
        "write_down(BiLSTM_Org_predictions.cpu(), test_df, BiLSTM_1in_org)\n",
        "\n",
        "cnn_out_loc = 'CNN_two_input_predictionScore.csv'\n",
        "write_down(cnn_test_predictions.cpu(), test_df, cnn_out_loc)\n",
        "\n",
        "cnn1_out_loc = 'CNN_ori_predictionScore.csv'\n",
        "write_down(cnn_ori_test_predictions.cpu(), test_df, cnn1_out_loc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "output finished, address: /content/baselineScore.csv\n",
            "output finished, address: /content/gru2in.csv\n",
            "output finished, address: /content/gru1in.csv\n",
            "output finished, address: /content/bilstm1in_org.csv\n",
            "output finished, address: /content/CNN_two_input_predictionScore.csv\n",
            "output finished, address: /content/CNN_ori_predictionScore.csv\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}